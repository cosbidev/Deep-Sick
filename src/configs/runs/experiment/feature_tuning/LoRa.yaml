peft: lora

fine_tune: False

rank_set: ${rank}

dropout: 0.1

alpha: 1

backbone_learning_rate: 1e-4

tuning_name: _LoRa_${rank}


