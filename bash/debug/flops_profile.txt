
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 6:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                             2       
data parallel size:                                                     2       
model parallel size:                                                    1       
batch size per GPU:                                                     2       
params per GPU:                                                         5.72 M  
params of model = params per GPU * mp_size:                             5.72 M  
fwd MACs per GPU:                                                       56.68 TMACs
fwd flops per GPU:                                                      113.38 T
fwd flops of model = fwd flops per GPU * mp_size:                       113.38 T
fwd latency:                                                            1.83 s  
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    61.95 TFLOPS
bwd latency:                                                            1.55 s  
bwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:                146.57 TFLOPS
fwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):      100.71 TFLOPS
step latency:                                                           184.84 ms
iter latency:                                                           3.56 s  
FLOPS per GPU = 3 * fwd flops per GPU / iter latency:                   95.48 TFLOPS
samples/second:                                                         1.12    

----------------------------- Aggregated Profile per GPU -----------------------------
Top 3 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'PeftModelForCausalLM': '5.72 M'}
    MACs        - {'PeftModelForCausalLM': '56.68 TMACs'}
    fwd latency - {'PeftModelForCausalLM': '1.75 s'}
depth 1:
    params      - {'LoraModel': '5.72 M'}
    MACs        - {'LoraModel': '56.68 TMACs'}
    fwd latency - {'LoraModel': '1.75 s'}
depth 2:
    params      - {'Gemma3ForConditionalGeneration': '5.72 M'}
    MACs        - {'Gemma3ForConditionalGeneration': '56.68 TMACs'}
    fwd latency - {'Gemma3ForConditionalGeneration': '1.75 s'}
depth 3:
    params      - {'Gemma3Model': '5.72 M', 'Linear': '0'}
    MACs        - {'Gemma3Model': '52.66 TMACs', 'Linear': '4.03 TMACs'}
    fwd latency - {'Gemma3Model': '1.72 s', 'Linear': '33.69 ms'}
depth 4:
    params      - {'Gemma3TextModel': '5.32 M', 'SiglipVisionModel': '399.6 K', 'Gemma3MultiModalProjector': '1.15 K'}
    MACs        - {'SiglipVisionModel': '30.18 TMACs', 'Gemma3TextModel': '22.47 TMACs', 'Gemma3MultiModalProjector': '3.02 GMACs'}
    fwd latency - {'Gemma3TextModel': '1.24 s', 'SiglipVisionModel': '460.79 ms', 'Gemma3MultiModalProjector': '1.16 ms'}
depth 5:
    params      - {'ModuleList': '5.31 M', 'SiglipVisionTransformer': '399.6 K', 'Gemma3RMSNorm': '3.71 K'}
    MACs        - {'SiglipVisionTransformer': '30.18 TMACs', 'ModuleList': '22.47 TMACs', 'Gemma3RotaryEmbedding': '1.54 MMACs'}
    fwd latency - {'ModuleList': '1.23 s', 'SiglipVisionTransformer': '460.6 ms', 'Gemma3RMSNorm': '919.58 us'}
depth 6:
    params      - {'Gemma3DecoderLayer': '5.31 M', 'SiglipEncoder': '396.14 K', 'LayerNorm': '2.3 K'}
    MACs        - {'SiglipEncoder': '30.17 TMACs', 'Gemma3DecoderLayer': '22.47 TMACs', 'SiglipVisionEmbeddings': '11.1 GMACs'}
    fwd latency - {'Gemma3DecoderLayer': '1.23 s', 'SiglipEncoder': '454.81 ms', 'SiglipVisionEmbeddings': '3.81 ms'}
depth 7:
    params      - {'Gemma3Attention': '4.97 M', 'ModuleList': '396.14 K', 'Gemma3RMSNorm': '348.16 K'}
    MACs        - {'ModuleList': '30.17 TMACs', 'Gemma3MLP': '16.54 TMACs', 'Gemma3Attention': '5.93 TMACs'}
    fwd latency - {'Gemma3MLP': '630.08 ms', 'Gemma3Attention': '494.69 ms', 'ModuleList': '440.75 ms'}
depth 8:
    params      - {'Linear': '4.95 M', 'SiglipEncoderLayer': '396.14 K', 'Gemma3RMSNorm': '17.41 K'}
    MACs        - {'SiglipEncoderLayer': '30.17 TMACs', 'Linear': '19.97 TMACs', 'Gemma3RMSNorm': '0 MACs'}
    fwd latency - {'Linear': '890.17 ms', 'SiglipEncoderLayer': '440.75 ms', 'Gemma3RMSNorm': '26.4 ms'}
depth 9:
    params      - {'ModuleDict': '4.95 M', 'SiglipMLP': '147.31 K', 'LayerNorm': '124.42 K'}
    MACs        - {'SiglipAttention': '21.4 TMACs', 'Linear': '19.25 TMACs', 'SiglipMLP': '8.77 TMACs'}
    fwd latency - {'SiglipAttention': '228.09 ms', 'Linear': '184.03 ms', 'SiglipMLP': '145.04 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

PeftModelForCausalLM(
  5.72 M = 100% Params, 56.68 TMACs = 100% MACs, 1.75 s = 100% latency, 64.78 TFLOPS
  (base_model): LoraModel(
    5.72 M = 100% Params, 56.68 TMACs = 100% MACs, 1.75 s = 100% latency, 64.78 TFLOPS
    (model): Gemma3ForConditionalGeneration(
      5.72 M = 100% Params, 56.68 TMACs = 100% MACs, 1.75 s = 100% latency, 64.78 TFLOPS
      (model): Gemma3Model(
        5.72 M = 100% Params, 52.66 TMACs = 92.89% MACs, 1.72 s = 98.07% latency, 61.36 TFLOPS
        (vision_tower): SiglipVisionModel(
          399.6 K = 6.99% Params, 30.18 TMACs = 53.24% MACs, 460.79 ms = 26.33% latency, 131.01 TFLOPS
          (vision_model): SiglipVisionTransformer(
            399.6 K = 6.99% Params, 30.18 TMACs = 53.24% MACs, 460.6 ms = 26.32% latency, 131.06 TFLOPS
            (embeddings): SiglipVisionEmbeddings(
              1.15 K = 0.02% Params, 11.1 GMACs = 0.02% MACs, 3.81 ms = 0.22% latency, 5.82 TFLOPS
              (patch_embedding): Conv2d(1.15 K = 0.02% Params, 11.1 GMACs = 0.02% MACs, 2.21 ms = 0.13% latency, 10.04 TFLOPS, 3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)
              (position_embedding): Embedding(0 = 0% Params, 0 MACs = 0% MACs, 209.09 us = 0.01% latency, 0 FLOPS, 4096, 1152)
            )
            (encoder): SiglipEncoder(
              396.14 K = 6.93% Params, 30.17 TMACs = 53.22% MACs, 454.81 ms = 25.99% latency, 132.68 TFLOPS
              (layers): ModuleList(
                (0): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.4 ms = 0.94% latency, 136.27 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 514.27 us = 0.03% latency, 183.51 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.51 ms = 0.49% latency, 186.23 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 635.62 us = 0.04% latency, 136.83 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 632.52 us = 0.04% latency, 137.5 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 679.49 us = 0.04% latency, 128 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 634.67 us = 0.04% latency, 137.04 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 492.33 us = 0.03% latency, 191.68 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.38 ms = 0.31% latency, 120.8 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 541.93 us = 0.03% latency, 130.12 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 160.08 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.61 ms = 0.09% latency, 201.91 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (1): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.25 ms = 0.93% latency, 137.57 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 493.53 us = 0.03% latency, 191.22 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.41 ms = 0.48% latency, 188.37 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 627.99 us = 0.04% latency, 138.49 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 627.28 us = 0.04% latency, 138.65 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 628.95 us = 0.04% latency, 138.28 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 638.01 us = 0.04% latency, 136.32 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 485.9 us = 0.03% latency, 194.22 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.35 ms = 0.31% latency, 121.42 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 532.15 us = 0.03% latency, 132.51 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.02 ms = 0.12% latency, 160.8 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.61 ms = 0.09% latency, 201.23 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (2): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.3 ms = 0.93% latency, 137.1 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 489 us = 0.03% latency, 192.99 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.44 ms = 0.48% latency, 187.75 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 626.33 us = 0.04% latency, 138.86 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 627.28 us = 0.04% latency, 138.65 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 626.56 us = 0.04% latency, 138.81 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 635.62 us = 0.04% latency, 136.83 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 485.66 us = 0.03% latency, 194.32 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.38 ms = 0.31% latency, 120.91 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 535.01 us = 0.03% latency, 131.8 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.02 ms = 0.12% latency, 160.81 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.62 ms = 0.09% latency, 200.46 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (3): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.29 ms = 0.93% latency, 137.24 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 483.27 us = 0.03% latency, 195.28 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.43 ms = 0.48% latency, 187.96 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 627.52 us = 0.04% latency, 138.6 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 625.61 us = 0.04% latency, 139.02 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 630.38 us = 0.04% latency, 137.97 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 632.05 us = 0.04% latency, 137.61 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 488.04 us = 0.03% latency, 193.37 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.38 ms = 0.31% latency, 120.79 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 528.81 us = 0.03% latency, 133.35 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 160.23 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.63 ms = 0.09% latency, 199.87 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (4): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.3 ms = 0.93% latency, 137.12 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 488.52 us = 0.03% latency, 193.18 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.42 ms = 0.48% latency, 188.14 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 634.67 us = 0.04% latency, 137.04 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 627.04 us = 0.04% latency, 138.7 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 630.14 us = 0.04% latency, 138.02 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 636.1 us = 0.04% latency, 136.73 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 494.24 us = 0.03% latency, 190.94 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.38 ms = 0.31% latency, 120.75 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 530 us = 0.03% latency, 133.05 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.02 ms = 0.12% latency, 160.91 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.62 ms = 0.09% latency, 200.69 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (5): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.32 ms = 0.93% latency, 136.95 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 498.29 us = 0.03% latency, 189.39 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.42 ms = 0.48% latency, 188.19 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.43 us = 0.04% latency, 138.18 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.43 us = 0.04% latency, 138.18 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 628.47 us = 0.04% latency, 138.39 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.81 us = 0.04% latency, 137.66 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 489.47 us = 0.03% latency, 192.8 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.39 ms = 0.31% latency, 120.49 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 530.48 us = 0.03% latency, 132.93 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.02 ms = 0.12% latency, 160.55 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.62 ms = 0.09% latency, 200.43 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (6): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.28 ms = 0.93% latency, 137.27 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 490.67 us = 0.03% latency, 192.33 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.42 ms = 0.48% latency, 188.33 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 627.99 us = 0.04% latency, 138.49 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.33 us = 0.04% latency, 137.76 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.9 us = 0.04% latency, 138.07 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 635.86 us = 0.04% latency, 136.78 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 492.81 us = 0.03% latency, 191.5 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.37 ms = 0.31% latency, 120.97 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 531.2 us = 0.03% latency, 132.75 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.02 ms = 0.12% latency, 160.7 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.63 ms = 0.09% latency, 199.4 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (7): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.35 ms = 0.93% latency, 136.67 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 496.15 us = 0.03% latency, 190.21 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.45 ms = 0.48% latency, 187.58 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 632.52 us = 0.04% latency, 137.5 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.81 us = 0.04% latency, 137.66 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 636.58 us = 0.04% latency, 136.63 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 637.77 us = 0.04% latency, 136.37 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 494.96 us = 0.03% latency, 190.67 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.38 ms = 0.31% latency, 120.73 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 529.77 us = 0.03% latency, 133.11 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 160.32 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.63 ms = 0.09% latency, 199.87 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (8): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.31 ms = 0.93% latency, 137.02 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 499.01 us = 0.03% latency, 189.12 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.42 ms = 0.48% latency, 188.11 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.43 us = 0.04% latency, 138.18 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.33 us = 0.04% latency, 137.76 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.81 us = 0.04% latency, 137.66 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 633.96 us = 0.04% latency, 137.19 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 497.82 us = 0.03% latency, 189.57 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.37 ms = 0.31% latency, 121.05 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 528.81 us = 0.03% latency, 133.35 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.02 ms = 0.12% latency, 160.51 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.62 ms = 0.09% latency, 200.31 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (9): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.37 ms = 0.94% latency, 136.52 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 496.86 us = 0.03% latency, 189.93 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.45 ms = 0.48% latency, 187.58 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 630.14 us = 0.04% latency, 138.02 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.57 us = 0.04% latency, 137.71 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.33 us = 0.04% latency, 137.76 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 634.19 us = 0.04% latency, 137.14 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 494.48 us = 0.03% latency, 190.85 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.41 ms = 0.31% latency, 120.1 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 530.48 us = 0.03% latency, 132.93 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 160.23 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.63 ms = 0.09% latency, 199.02 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (10): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.39 ms = 0.94% latency, 136.4 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 493.53 us = 0.03% latency, 191.22 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.5 ms = 0.49% latency, 186.44 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.19 us = 0.04% latency, 138.23 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 635.62 us = 0.04% latency, 136.83 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 638.48 us = 0.04% latency, 136.22 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 637.29 us = 0.04% latency, 136.47 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 491.86 us = 0.03% latency, 191.87 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.39 ms = 0.31% latency, 120.62 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 530 us = 0.03% latency, 133.05 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 160.42 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.63 ms = 0.09% latency, 199.17 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (11): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.34 ms = 0.93% latency, 136.76 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 500.44 us = 0.03% latency, 188.58 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.45 ms = 0.48% latency, 187.64 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.33 us = 0.04% latency, 137.76 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 628.95 us = 0.04% latency, 138.28 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 633 us = 0.04% latency, 137.4 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 633.96 us = 0.04% latency, 137.19 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 495.67 us = 0.03% latency, 190.39 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.39 ms = 0.31% latency, 120.62 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 530.72 us = 0.03% latency, 132.87 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.02 ms = 0.12% latency, 160.68 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.63 ms = 0.09% latency, 199.63 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (12): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.3 ms = 0.93% latency, 137.12 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 492.81 us = 0.03% latency, 191.5 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.43 ms = 0.48% latency, 187.98 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 642.3 us = 0.04% latency, 135.41 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 625.61 us = 0.04% latency, 139.02 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.9 us = 0.04% latency, 138.07 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 636.34 us = 0.04% latency, 136.68 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 490.43 us = 0.03% latency, 192.43 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.38 ms = 0.31% latency, 120.9 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 531.91 us = 0.03% latency, 132.57 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.02 ms = 0.12% latency, 160.78 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.63 ms = 0.09% latency, 198.88 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (13): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.33 ms = 0.93% latency, 136.89 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 495.2 us = 0.03% latency, 190.57 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.44 ms = 0.48% latency, 187.77 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 632.05 us = 0.04% latency, 137.61 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 635.86 us = 0.04% latency, 136.78 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.57 us = 0.04% latency, 137.71 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 633.24 us = 0.04% latency, 137.35 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 493.76 us = 0.03% latency, 191.13 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.39 ms = 0.31% latency, 120.63 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 531.2 us = 0.03% latency, 132.75 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.02 ms = 0.12% latency, 160.61 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.62 ms = 0.09% latency, 200.25 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (14): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.29 ms = 0.93% latency, 137.21 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 491.38 us = 0.03% latency, 192.05 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.42 ms = 0.48% latency, 188.23 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 627.52 us = 0.04% latency, 138.6 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 630.86 us = 0.04% latency, 137.87 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.33 us = 0.04% latency, 137.76 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 638.01 us = 0.04% latency, 136.32 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 493.53 us = 0.03% latency, 191.22 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.38 ms = 0.31% latency, 120.9 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 530.24 us = 0.03% latency, 132.99 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.02 ms = 0.12% latency, 160.61 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.63 ms = 0.09% latency, 199.69 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (15): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.32 ms = 0.93% latency, 136.97 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 502.35 us = 0.03% latency, 187.86 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.43 ms = 0.48% latency, 188.01 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.66 us = 0.04% latency, 138.13 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 628.47 us = 0.04% latency, 138.39 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.43 us = 0.04% latency, 138.18 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 632.52 us = 0.04% latency, 137.5 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 487.57 us = 0.03% latency, 193.56 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.39 ms = 0.31% latency, 120.65 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 537.16 us = 0.03% latency, 131.28 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 160.28 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.63 ms = 0.09% latency, 199.93 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (16): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.36 ms = 0.93% latency, 136.59 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 493.05 us = 0.03% latency, 191.4 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.45 ms = 0.48% latency, 187.56 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.9 us = 0.04% latency, 138.07 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.43 us = 0.04% latency, 138.18 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.09 us = 0.04% latency, 137.81 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 634.19 us = 0.04% latency, 137.14 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 494.96 us = 0.03% latency, 190.67 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.41 ms = 0.31% latency, 120.14 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 530.48 us = 0.03% latency, 132.93 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 159.72 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.63 ms = 0.09% latency, 199.25 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (17): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.4 ms = 0.94% latency, 136.31 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 494.72 us = 0.03% latency, 190.76 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.49 ms = 0.49% latency, 186.6 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.57 us = 0.04% latency, 137.71 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.33 us = 0.04% latency, 137.76 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 639.2 us = 0.04% latency, 136.07 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 638.72 us = 0.04% latency, 136.17 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 489.23 us = 0.03% latency, 192.9 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.39 ms = 0.31% latency, 120.48 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 530.24 us = 0.03% latency, 132.99 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.04 ms = 0.12% latency, 159.46 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.62 ms = 0.09% latency, 200.49 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (18): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.32 ms = 0.93% latency, 136.94 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 495.43 us = 0.03% latency, 190.48 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.44 ms = 0.48% latency, 187.87 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.43 us = 0.04% latency, 138.18 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 625.13 us = 0.04% latency, 139.13 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 632.52 us = 0.04% latency, 137.5 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 632.05 us = 0.04% latency, 137.61 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 492.57 us = 0.03% latency, 191.59 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.38 ms = 0.31% latency, 120.73 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 530.48 us = 0.03% latency, 132.93 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 159.68 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.61 ms = 0.09% latency, 201.7 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (19): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.28 ms = 0.93% latency, 137.3 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 491.38 us = 0.03% latency, 192.05 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.44 ms = 0.48% latency, 187.79 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 637.53 us = 0.04% latency, 136.42 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.19 us = 0.04% latency, 138.23 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.43 us = 0.04% latency, 138.18 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 634.43 us = 0.04% latency, 137.09 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 494.96 us = 0.03% latency, 190.67 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.34 ms = 0.31% latency, 121.75 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 530.24 us = 0.03% latency, 132.99 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 160.25 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.59 ms = 0.09% latency, 204.92 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (20): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.3 ms = 0.93% latency, 137.08 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 490.19 us = 0.03% latency, 192.52 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.46 ms = 0.48% latency, 187.41 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 628.23 us = 0.04% latency, 138.44 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 635.39 us = 0.04% latency, 136.88 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 630.14 us = 0.04% latency, 138.02 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 637.77 us = 0.04% latency, 136.37 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 492.33 us = 0.03% latency, 191.68 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.35 ms = 0.31% latency, 121.55 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 528.1 us = 0.03% latency, 133.53 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.04 ms = 0.12% latency, 159.67 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.58 ms = 0.09% latency, 205.57 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (21): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.37 ms = 0.94% latency, 136.5 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 500.92 us = 0.03% latency, 188.4 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.47 ms = 0.48% latency, 187.02 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 632.05 us = 0.04% latency, 137.61 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 633.96 us = 0.04% latency, 137.19 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.81 us = 0.04% latency, 137.66 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 637.29 us = 0.04% latency, 136.47 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 496.63 us = 0.03% latency, 190.03 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.36 ms = 0.31% latency, 121.29 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 529.29 us = 0.03% latency, 133.23 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 159.87 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.58 ms = 0.09% latency, 205.47 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (22): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.3 ms = 0.93% latency, 137.11 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 503.3 us = 0.03% latency, 187.51 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.47 ms = 0.48% latency, 187.08 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 633.48 us = 0.04% latency, 137.29 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 634.43 us = 0.04% latency, 137.09 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 635.39 us = 0.04% latency, 136.88 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 641.35 us = 0.04% latency, 135.61 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 494.48 us = 0.03% latency, 190.85 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.31 ms = 0.3% latency, 122.41 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 527.86 us = 0.03% latency, 133.59 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 160.34 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.57 ms = 0.09% latency, 207.44 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (23): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.32 ms = 0.93% latency, 136.93 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 495.67 us = 0.03% latency, 190.39 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.46 ms = 0.48% latency, 187.33 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 634.43 us = 0.04% latency, 137.09 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.9 us = 0.04% latency, 138.07 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 635.86 us = 0.04% latency, 136.78 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 637.29 us = 0.04% latency, 136.47 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 489.71 us = 0.03% latency, 192.71 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.34 ms = 0.31% latency, 121.73 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 535.49 us = 0.03% latency, 131.69 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 160.21 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.57 ms = 0.09% latency, 207.25 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (24): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.3 ms = 0.93% latency, 137.09 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 499.01 us = 0.03% latency, 189.12 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.47 ms = 0.48% latency, 187.21 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 630.86 us = 0.04% latency, 137.87 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 630.14 us = 0.04% latency, 138.02 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 632.29 us = 0.04% latency, 137.55 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 634.43 us = 0.04% latency, 137.09 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 495.2 us = 0.03% latency, 190.57 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.33 ms = 0.3% latency, 122.05 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 529.77 us = 0.03% latency, 133.11 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 160.36 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.56 ms = 0.09% latency, 207.85 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (25): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.29 ms = 0.93% latency, 137.19 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 493.29 us = 0.03% latency, 191.31 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.44 ms = 0.48% latency, 187.82 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 630.14 us = 0.04% latency, 138.02 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 627.76 us = 0.04% latency, 138.55 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 629.66 us = 0.04% latency, 138.13 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 631.09 us = 0.04% latency, 137.81 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 491.38 us = 0.03% latency, 192.05 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.35 ms = 0.31% latency, 121.45 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 531.44 us = 0.03% latency, 132.69 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 160.3 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.57 ms = 0.09% latency, 206.66 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
                (26): SiglipEncoderLayer(
                  14.67 K = 0.26% Params, 1.12 TMACs = 1.97% MACs, 16.36 ms = 0.93% latency, 136.6 TFLOPS
                  (layer_norm1): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 500.44 us = 0.03% latency, 188.58 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (self_attn): SiglipAttention(
                    4.61 K = 0.08% Params, 792.42 GMACs = 1.4% MACs, 8.45 ms = 0.48% latency, 187.45 TFLOPS
                    (k_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 627.99 us = 0.04% latency, 138.49 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (v_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 630.14 us = 0.04% latency, 138.02 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (q_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 630.62 us = 0.04% latency, 137.92 TFLOPS, in_features=1152, out_features=1152, bias=True)
                    (out_proj): Linear(1.15 K = 0.02% Params, 43.49 GMACs = 0.08% MACs, 636.34 us = 0.04% latency, 136.68 TFLOPS, in_features=1152, out_features=1152, bias=True)
                  )
                  (layer_norm2): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 497.58 us = 0.03% latency, 189.66 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
                  (mlp): SiglipMLP(
                    5.46 K = 0.1% Params, 324.94 GMACs = 0.57% MACs, 5.38 ms = 0.31% latency, 120.9 TFLOPS
                    (activation_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 531.44 us = 0.03% latency, 132.69 GFLOPS)
                    (fc1): Linear(4.3 K = 0.08% Params, 162.47 GMACs = 0.29% MACs, 2.03 ms = 0.12% latency, 159.81 TFLOPS, in_features=1152, out_features=4304, bias=True)
                    (fc2): Linear(1.15 K = 0.02% Params, 162.47 GMACs = 0.29% MACs, 1.6 ms = 0.09% latency, 203.48 TFLOPS, in_features=4304, out_features=1152, bias=True)
                  )
                )
              )
            )
            (post_layernorm): LayerNorm(2.3 K = 0.04% Params, 0 MACs = 0% MACs, 506.4 us = 0.03% latency, 186.36 GFLOPS, (1152,), eps=1e-06, elementwise_affine=True)
          )
        )
        (multi_modal_projector): Gemma3MultiModalProjector(
          1.15 K = 0.02% Params, 3.02 GMACs = 0.01% MACs, 1.16 ms = 0.07% latency, 5.21 TFLOPS
          (mm_soft_emb_norm): Gemma3RMSNorm(1.15 K = 0.02% Params, 0 MACs = 0% MACs, 317.57 us = 0.02% latency, 0 FLOPS, (1152,), eps=1e-06)
          (avg_pool): AvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 120.16 us = 0.01% latency, 157.07 GFLOPS, kernel_size=4, stride=4, padding=0)
        )
        (language_model): Gemma3TextModel(
          5.32 M = 92.99% Params, 22.47 TMACs = 39.65% MACs, 1.24 s = 70.8% latency, 36.27 TFLOPS
          (embed_tokens): Gemma3TextScaledWordEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 342.37 us = 0.02% latency, 0 FLOPS, 262208, 2560, padding_idx=0)
          (layers): ModuleList(
            (0): Gemma3DecoderLayer(
              633.34 K = 11.08% Params, 661 GMACs = 1.17% MACs, 34.35 ms = 1.96% latency, 38.49 TFLOPS
              (self_attn): Gemma3Attention(
                623.1 K = 10.9% Params, 174.39 GMACs = 0.31% MACs, 14.3 ms = 0.82% latency, 24.39 TFLOPS
                (q_proj): lora.Linear(
                  163.84 K = 2.87% Params, 33.23 GMACs = 0.06% MACs, 2.54 ms = 0.15% latency, 26.15 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 408.89 us = 0.02% latency, 153.87 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 153.06 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 153.06 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    163.84 K = 2.87% Params, 983.04 MMACs = 0% MACs, 187.64 us = 0.01% latency, 10.48 TFLOPS
                    (default): Linear(163.84 K = 2.87% Params, 983.04 MMACs = 0% MACs, 187.64 us = 0.01% latency, 10.48 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 252.01 us = 0.01% latency, 6.24 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 252.01 us = 0.01% latency, 6.24 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  229.38 K = 4.01% Params, 17.1 GMACs = 0.03% MACs, 1.93 ms = 0.11% latency, 17.7 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 257.02 us = 0.01% latency, 122.39 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.18 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.18 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    163.84 K = 2.87% Params, 983.04 MMACs = 0% MACs, 175.24 us = 0.01% latency, 11.22 TFLOPS
                    (default): Linear(163.84 K = 2.87% Params, 983.04 MMACs = 0% MACs, 175.24 us = 0.01% latency, 11.22 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 159.74 us = 0.01% latency, 4.92 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 159.74 us = 0.01% latency, 4.92 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  229.38 K = 4.01% Params, 17.1 GMACs = 0.03% MACs, 2.21 ms = 0.13% latency, 15.46 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 254.87 us = 0.01% latency, 123.43 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    163.84 K = 2.87% Params, 983.04 MMACs = 0% MACs, 177.38 us = 0.01% latency, 11.08 TFLOPS
                    (default): Linear(163.84 K = 2.87% Params, 983.04 MMACs = 0% MACs, 177.38 us = 0.01% latency, 11.08 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 164.27 us = 0.01% latency, 4.79 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 164.27 us = 0.01% latency, 4.79 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.37 ms = 0.25% latency, 15.22 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 353.1 us = 0.02% latency, 178.18 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 136.85 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 136.85 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 177.38 us = 0.01% latency, 8.87 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 177.38 us = 0.01% latency, 8.87 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 255.58 us = 0.01% latency, 7.69 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 255.58 us = 0.01% latency, 7.69 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 502.11 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 274.42 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.82 ms = 0.96% latency, 57.86 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.81 ms = 0.33% latency, 55.88 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 222.09 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 177.86 us = 0.01% latency, 11.05 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 177.86 us = 0.01% latency, 11.05 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 254.15 us = 0.01% latency, 30.94 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 254.15 us = 0.01% latency, 30.94 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.68 ms = 0.21% latency, 88.11 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.55 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 132.08 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 132.08 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.85 us = 0.01% latency, 11.37 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.85 us = 0.01% latency, 11.37 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 244.38 us = 0.01% latency, 32.18 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 244.38 us = 0.01% latency, 32.18 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.88 ms = 0.22% latency, 83.51 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 236.92 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 325.92 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 325.92 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 201.94 us = 0.01% latency, 38.94 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 201.94 us = 0.01% latency, 38.94 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 246.29 us = 0.01% latency, 7.98 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 246.29 us = 0.01% latency, 7.98 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 252.96 us = 0.01% latency, 121.44 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 613.21 us = 0.04% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.31 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 603.2 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.78 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (1): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 35.13 ms = 2.01% latency, 37.63 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 15.15 ms = 0.87% latency, 23.03 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.51 ms = 0.14% latency, 26.5 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 403.4 us = 0.02% latency, 155.96 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 142.57 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 142.57 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 178.1 us = 0.01% latency, 11.04 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 178.1 us = 0.01% latency, 11.04 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 244.86 us = 0.01% latency, 6.42 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 244.86 us = 0.01% latency, 6.42 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.56 ms = 0.15% latency, 13.35 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 255.58 us = 0.01% latency, 123.08 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 147.1 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 147.1 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 179.53 us = 0.01% latency, 10.95 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 179.53 us = 0.01% latency, 10.95 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 166.65 us = 0.01% latency, 4.72 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 166.65 us = 0.01% latency, 4.72 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.22 ms = 0.13% latency, 15.42 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 258.92 us = 0.01% latency, 121.49 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 134.23 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 134.23 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.95 us = 0.01% latency, 11.17 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.95 us = 0.01% latency, 11.17 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 163.56 us = 0.01% latency, 4.81 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 163.56 us = 0.01% latency, 4.81 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.34 ms = 0.25% latency, 15.32 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 346.9 us = 0.02% latency, 181.36 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 176.67 us = 0.01% latency, 8.9 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 176.67 us = 0.01% latency, 8.9 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 250.1 us = 0.01% latency, 7.86 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 250.1 us = 0.01% latency, 7.86 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 502.35 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 273.7 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.78 ms = 0.96% latency, 57.99 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.78 ms = 0.33% latency, 56.1 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 221.71 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.04 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.04 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 183.58 us = 0.01% latency, 10.71 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 183.58 us = 0.01% latency, 10.71 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 251.05 us = 0.01% latency, 31.33 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 251.05 us = 0.01% latency, 31.33 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.69 ms = 0.21% latency, 87.83 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.87 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.71 us = 0.01% latency, 11.19 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.71 us = 0.01% latency, 11.19 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 253.92 us = 0.01% latency, 30.97 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 253.92 us = 0.01% latency, 30.97 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.87 ms = 0.22% latency, 83.87 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.26 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 326.16 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 326.16 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 205.28 us = 0.01% latency, 38.31 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 205.28 us = 0.01% latency, 38.31 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 254.39 us = 0.01% latency, 7.73 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 254.39 us = 0.01% latency, 7.73 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 251.77 us = 0.01% latency, 122.02 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 603.44 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.31 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.72 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.11 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (2): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.62 ms = 1.98% latency, 38.19 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.6 ms = 0.83% latency, 23.89 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.51 ms = 0.14% latency, 26.48 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 407.46 us = 0.02% latency, 154.41 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 132.8 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 132.8 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 178.58 us = 0.01% latency, 11.01 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 178.58 us = 0.01% latency, 11.01 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 258.68 us = 0.01% latency, 6.08 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 258.68 us = 0.01% latency, 6.08 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.01 ms = 0.11% latency, 17.01 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 265.12 us = 0.02% latency, 118.65 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 179.77 us = 0.01% latency, 10.94 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 179.77 us = 0.01% latency, 10.94 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 170.47 us = 0.01% latency, 4.61 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 170.47 us = 0.01% latency, 4.61 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.23 ms = 0.13% latency, 15.32 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 253.68 us = 0.01% latency, 124.01 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.66 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.66 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 179.29 us = 0.01% latency, 10.97 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 179.29 us = 0.01% latency, 10.97 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 168.32 us = 0.01% latency, 4.67 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 168.32 us = 0.01% latency, 4.67 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.35 ms = 0.25% latency, 15.26 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 347.38 us = 0.02% latency, 181.11 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.28 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.28 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 178.34 us = 0.01% latency, 8.82 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 178.34 us = 0.01% latency, 8.82 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 257.73 us = 0.01% latency, 7.63 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 257.73 us = 0.01% latency, 7.63 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 503.54 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.75 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.81 ms = 0.96% latency, 57.89 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.8 ms = 0.33% latency, 55.95 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 221.3 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.9 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.9 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.09 us = 0.01% latency, 11.36 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.09 us = 0.01% latency, 11.36 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 251.29 us = 0.01% latency, 31.3 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 251.29 us = 0.01% latency, 31.3 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.72 ms = 0.21% latency, 87.12 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 221.82 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 136.38 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 136.38 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 179.05 us = 0.01% latency, 10.98 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 179.05 us = 0.01% latency, 10.98 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 254.15 us = 0.01% latency, 30.94 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 254.15 us = 0.01% latency, 30.94 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.85 ms = 0.22% latency, 84.34 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 236.75 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 323.77 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 323.77 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 197.17 us = 0.01% latency, 39.89 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 197.17 us = 0.01% latency, 39.89 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 245.57 us = 0.01% latency, 8.01 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 245.57 us = 0.01% latency, 8.01 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 252.49 us = 0.01% latency, 121.67 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 605.11 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.35 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 598.91 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.78 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (3): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.64 ms = 1.98% latency, 38.17 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.65 ms = 0.84% latency, 23.8 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.62 ms = 0.15% latency, 25.38 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 405.79 us = 0.02% latency, 155.04 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 141.14 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 141.14 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 174.28 us = 0.01% latency, 11.28 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 174.28 us = 0.01% latency, 11.28 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 355.72 us = 0.02% latency, 4.42 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 355.72 us = 0.02% latency, 4.42 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2 ms = 0.11% latency, 17.11 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 265.6 us = 0.02% latency, 118.44 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 134.71 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 134.71 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.81 us = 0.01% latency, 11.31 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.81 us = 0.01% latency, 11.31 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 163.08 us = 0.01% latency, 4.82 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 163.08 us = 0.01% latency, 4.82 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.21 ms = 0.13% latency, 15.51 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 262.98 us = 0.02% latency, 119.62 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 136.38 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 136.38 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.07 us = 0.01% latency, 4.98 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.07 us = 0.01% latency, 4.98 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.32 ms = 0.25% latency, 15.38 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 348.57 us = 0.02% latency, 180.49 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 171.66 us = 0.01% latency, 9.16 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 171.66 us = 0.01% latency, 9.16 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 255.58 us = 0.01% latency, 7.69 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 255.58 us = 0.01% latency, 7.69 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 504.73 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 276.09 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.77 ms = 0.96% latency, 58.02 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.8 ms = 0.33% latency, 55.98 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 221.82 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 134.23 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 134.23 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 261.07 us = 0.01% latency, 30.12 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 261.07 us = 0.01% latency, 30.12 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.69 ms = 0.21% latency, 87.98 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 221.86 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 167.85 us = 0.01% latency, 11.71 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 167.85 us = 0.01% latency, 11.71 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.96 us = 0.01% latency, 31.72 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.96 us = 0.01% latency, 31.72 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.86 ms = 0.22% latency, 84.12 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.82 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 333.55 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 333.55 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.56 us = 0.01% latency, 39.41 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.56 us = 0.01% latency, 39.41 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 255.82 us = 0.01% latency, 7.69 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 255.82 us = 0.01% latency, 7.69 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 252.96 us = 0.01% latency, 121.44 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 607.01 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 612.02 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.48 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 608.44 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (4): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.87 ms = 1.99% latency, 37.91 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.53 ms = 0.83% latency, 24 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.52 ms = 0.14% latency, 26.39 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 405.31 us = 0.02% latency, 155.23 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 137.81 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 137.81 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 184.77 us = 0.01% latency, 10.64 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 184.77 us = 0.01% latency, 10.64 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 250.1 us = 0.01% latency, 6.29 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 250.1 us = 0.01% latency, 6.29 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2 ms = 0.11% latency, 17.12 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 262.5 us = 0.01% latency, 119.84 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.81 us = 0.01% latency, 11.31 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.81 us = 0.01% latency, 11.31 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.17 us = 0.01% latency, 4.88 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.17 us = 0.01% latency, 4.88 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.19 ms = 0.13% latency, 15.6 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 255.35 us = 0.01% latency, 123.19 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.28 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.28 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 180.01 us = 0.01% latency, 10.92 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 180.01 us = 0.01% latency, 10.92 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.17 us = 0.01% latency, 4.88 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.17 us = 0.01% latency, 4.88 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.32 ms = 0.25% latency, 15.37 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 346.66 us = 0.02% latency, 181.49 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.03 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.03 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 176.67 us = 0.01% latency, 8.9 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 176.67 us = 0.01% latency, 8.9 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 247.48 us = 0.01% latency, 7.94 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 247.48 us = 0.01% latency, 7.94 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 502.11 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 277.04 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 17.12 ms = 0.98% latency, 56.84 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.79 ms = 0.33% latency, 56.07 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 221.94 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 132.8 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 132.8 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 184.06 us = 0.01% latency, 10.68 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 184.06 us = 0.01% latency, 10.68 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.58 us = 0.01% latency, 31.38 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.58 us = 0.01% latency, 31.38 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.69 ms = 0.21% latency, 87.82 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 221.56 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 137.57 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 137.57 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.38 us = 0.01% latency, 11.41 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.38 us = 0.01% latency, 11.41 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 249.39 us = 0.01% latency, 31.53 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 249.39 us = 0.01% latency, 31.53 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 4.22 ms = 0.24% latency, 76.95 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.05 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 328.06 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 328.06 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 201.46 us = 0.01% latency, 39.04 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 201.46 us = 0.01% latency, 39.04 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 298.02 us = 0.02% latency, 6.6 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 298.02 us = 0.02% latency, 6.6 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 250.82 us = 0.01% latency, 122.48 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.72 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.83 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 600.1 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 612.74 us = 0.04% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (5): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.54 ms = 1.97% latency, 38.28 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.56 ms = 0.83% latency, 23.95 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.51 ms = 0.14% latency, 26.45 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 405.79 us = 0.02% latency, 155.04 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.46 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.46 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 185.49 us = 0.01% latency, 10.6 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 185.49 us = 0.01% latency, 10.6 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 249.86 us = 0.01% latency, 6.29 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 249.86 us = 0.01% latency, 6.29 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.96 ms = 0.11% latency, 17.41 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 257.97 us = 0.01% latency, 121.94 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.79 us = 0.01% latency, 4.95 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.79 us = 0.01% latency, 4.95 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.23 ms = 0.13% latency, 15.34 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 258.21 us = 0.01% latency, 121.83 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.28 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.28 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 184.3 us = 0.01% latency, 10.67 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 184.3 us = 0.01% latency, 10.67 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 163.79 us = 0.01% latency, 4.8 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 163.79 us = 0.01% latency, 4.8 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.32 ms = 0.25% latency, 15.37 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 347.85 us = 0.02% latency, 180.87 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 174.76 us = 0.01% latency, 9 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 174.76 us = 0.01% latency, 9 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 244.62 us = 0.01% latency, 8.04 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 244.62 us = 0.01% latency, 8.04 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 502.59 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 275.61 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.76 ms = 0.96% latency, 58.08 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.78 ms = 0.33% latency, 56.14 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 221.82 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.94 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.94 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 182.39 us = 0.01% latency, 10.78 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 182.39 us = 0.01% latency, 10.78 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 252.01 us = 0.01% latency, 31.21 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 252.01 us = 0.01% latency, 31.21 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.69 ms = 0.21% latency, 87.87 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 221.79 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 137.33 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 137.33 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.24 us = 0.01% latency, 11.22 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.24 us = 0.01% latency, 11.22 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.52 us = 0.01% latency, 31.9 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.52 us = 0.01% latency, 31.9 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.85 ms = 0.22% latency, 84.24 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 236.96 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 320.2 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 320.2 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 197.41 us = 0.01% latency, 39.84 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 197.41 us = 0.01% latency, 39.84 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 246.29 us = 0.01% latency, 7.98 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 246.29 us = 0.01% latency, 7.98 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 250.34 us = 0.01% latency, 122.71 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 605.82 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.78 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.25 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 612.02 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (6): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.5 ms = 1.97% latency, 38.31 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.51 ms = 0.83% latency, 24.04 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.51 ms = 0.14% latency, 26.5 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 404.36 us = 0.02% latency, 155.59 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 134.71 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 134.71 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 174.05 us = 0.01% latency, 11.3 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 174.05 us = 0.01% latency, 11.3 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 247.24 us = 0.01% latency, 6.36 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 247.24 us = 0.01% latency, 6.36 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.97 ms = 0.11% latency, 17.38 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 262.5 us = 0.01% latency, 119.84 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.57 us = 0.01% latency, 11.33 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.57 us = 0.01% latency, 11.33 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 156.64 us = 0.01% latency, 5.02 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 156.64 us = 0.01% latency, 5.02 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.2 ms = 0.13% latency, 15.52 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 253.68 us = 0.01% latency, 124.01 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.41 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.41 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.09 us = 0.01% latency, 11.36 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.09 us = 0.01% latency, 11.36 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 163.32 us = 0.01% latency, 4.82 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 163.32 us = 0.01% latency, 4.82 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.32 ms = 0.25% latency, 15.39 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 347.38 us = 0.02% latency, 181.11 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 132.32 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 132.32 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 176.19 us = 0.01% latency, 8.93 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 176.19 us = 0.01% latency, 8.93 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 253.44 us = 0.01% latency, 7.76 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 253.44 us = 0.01% latency, 7.76 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 500.68 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 273.7 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.81 ms = 0.96% latency, 57.89 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.87 ms = 0.34% latency, 55.3 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 221.71 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.81 us = 0.01% latency, 11.31 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.81 us = 0.01% latency, 11.31 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 251.29 us = 0.01% latency, 31.3 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 251.29 us = 0.01% latency, 31.3 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.67 ms = 0.21% latency, 88.41 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 222.2 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 184.54 us = 0.01% latency, 10.65 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 184.54 us = 0.01% latency, 10.65 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 248.43 us = 0.01% latency, 31.66 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 248.43 us = 0.01% latency, 31.66 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.86 ms = 0.22% latency, 84.14 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 236.54 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 324.01 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 324.01 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 200.51 us = 0.01% latency, 39.22 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 200.51 us = 0.01% latency, 39.22 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 243.9 us = 0.01% latency, 8.06 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 243.9 us = 0.01% latency, 8.06 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 253.68 us = 0.01% latency, 121.1 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.96 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 613.45 us = 0.04% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 599.86 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.35 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (7): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.46 ms = 1.97% latency, 38.37 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.52 ms = 0.83% latency, 24.02 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.48 ms = 0.14% latency, 26.79 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 406.03 us = 0.02% latency, 154.95 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.66 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.66 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 177.38 us = 0.01% latency, 11.08 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 177.38 us = 0.01% latency, 11.08 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 247.72 us = 0.01% latency, 6.35 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 247.72 us = 0.01% latency, 6.35 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.99 ms = 0.11% latency, 17.17 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 262.74 us = 0.02% latency, 119.73 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 174.52 us = 0.01% latency, 11.27 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 174.52 us = 0.01% latency, 11.27 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 166.65 us = 0.01% latency, 4.72 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 166.65 us = 0.01% latency, 4.72 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.21 ms = 0.13% latency, 15.51 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 259.64 us = 0.01% latency, 121.16 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 139.95 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 139.95 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 179.77 us = 0.01% latency, 10.94 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 179.77 us = 0.01% latency, 10.94 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.65 us = 0.01% latency, 4.87 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.65 us = 0.01% latency, 4.87 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.35 ms = 0.25% latency, 15.29 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 343.08 us = 0.02% latency, 183.38 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 191.45 us = 0.01% latency, 8.22 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 191.45 us = 0.01% latency, 8.22 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 248.67 us = 0.01% latency, 7.91 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 248.67 us = 0.01% latency, 7.91 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 503.78 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 276.8 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.75 ms = 0.96% latency, 58.11 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.79 ms = 0.33% latency, 56.04 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.54 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 141.86 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 141.86 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 176.91 us = 0.01% latency, 11.11 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 176.91 us = 0.01% latency, 11.11 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.1 us = 0.01% latency, 31.44 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.1 us = 0.01% latency, 31.44 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.7 ms = 0.21% latency, 87.57 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.25 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.66 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.66 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 174.05 us = 0.01% latency, 11.3 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 174.05 us = 0.01% latency, 11.3 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 260.11 us = 0.01% latency, 30.23 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 260.11 us = 0.01% latency, 30.23 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.84 ms = 0.22% latency, 84.52 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.48 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 326.16 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 326.16 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.79 us = 0.01% latency, 39.36 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.79 us = 0.01% latency, 39.36 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 248.91 us = 0.01% latency, 7.9 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 248.91 us = 0.01% latency, 7.9 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 250.58 us = 0.01% latency, 122.6 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 601.29 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.78 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 597.95 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 612.97 us = 0.04% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (8): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.47 ms = 1.97% latency, 38.36 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.47 ms = 0.83% latency, 24.1 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.48 ms = 0.14% latency, 26.79 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 405.55 us = 0.02% latency, 155.13 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 246.05 us = 0.01% latency, 6.39 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 246.05 us = 0.01% latency, 6.39 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.99 ms = 0.11% latency, 17.2 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 258.68 us = 0.01% latency, 121.6 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 140.19 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 140.19 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 174.05 us = 0.01% latency, 11.3 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 174.05 us = 0.01% latency, 11.3 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.93 us = 0.01% latency, 5.04 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.93 us = 0.01% latency, 5.04 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.17 ms = 0.12% latency, 15.75 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 251.05 us = 0.01% latency, 125.3 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 168.09 us = 0.01% latency, 11.7 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 168.09 us = 0.01% latency, 11.7 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 166.42 us = 0.01% latency, 4.73 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 166.42 us = 0.01% latency, 4.73 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.32 ms = 0.25% latency, 15.37 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 352.86 us = 0.02% latency, 178.3 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 175 us = 0.01% latency, 8.99 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 175 us = 0.01% latency, 8.99 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 252.01 us = 0.01% latency, 7.8 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 252.01 us = 0.01% latency, 7.8 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 508.79 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 276.33 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.77 ms = 0.96% latency, 58.03 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.79 ms = 0.33% latency, 55.99 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.35 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.42 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.42 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 178.1 us = 0.01% latency, 11.04 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 178.1 us = 0.01% latency, 11.04 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.48 us = 0.01% latency, 31.78 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.48 us = 0.01% latency, 31.78 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.66 ms = 0.21% latency, 88.68 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.59 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 176.67 us = 0.01% latency, 11.13 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 176.67 us = 0.01% latency, 11.13 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 243.43 us = 0.01% latency, 32.31 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 243.43 us = 0.01% latency, 32.31 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.89 ms = 0.22% latency, 83.43 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.05 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 324.96 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 324.96 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 201.46 us = 0.01% latency, 39.04 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 201.46 us = 0.01% latency, 39.04 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 247 us = 0.01% latency, 7.96 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 247 us = 0.01% latency, 7.96 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 252.49 us = 0.01% latency, 121.67 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 607.49 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.64 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 603.91 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.59 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (9): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.31 ms = 1.96% latency, 38.53 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.52 ms = 0.83% latency, 24.02 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.52 ms = 0.14% latency, 26.41 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 409.36 us = 0.02% latency, 153.69 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 138.52 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 138.52 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 183.82 us = 0.01% latency, 10.7 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 183.82 us = 0.01% latency, 10.7 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 253.68 us = 0.01% latency, 6.2 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 253.68 us = 0.01% latency, 6.2 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.98 ms = 0.11% latency, 17.26 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 257.25 us = 0.01% latency, 122.28 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 160.69 us = 0.01% latency, 4.89 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 160.69 us = 0.01% latency, 4.89 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.21 ms = 0.13% latency, 15.47 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 258.68 us = 0.01% latency, 121.6 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 176.43 us = 0.01% latency, 11.14 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 176.43 us = 0.01% latency, 11.14 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 160.93 us = 0.01% latency, 4.89 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 160.93 us = 0.01% latency, 4.89 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.33 ms = 0.25% latency, 15.36 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 351.43 us = 0.02% latency, 179.02 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 139.24 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 139.24 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 173.57 us = 0.01% latency, 9.06 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 173.57 us = 0.01% latency, 9.06 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 243.19 us = 0.01% latency, 8.08 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 243.19 us = 0.01% latency, 8.08 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 503.06 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 274.42 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.59 ms = 0.95% latency, 58.65 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.78 ms = 0.33% latency, 56.14 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.54 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 132.8 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 132.8 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.81 us = 0.01% latency, 11.31 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.81 us = 0.01% latency, 11.31 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.1 us = 0.01% latency, 31.44 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.1 us = 0.01% latency, 31.44 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.68 ms = 0.21% latency, 88.15 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.21 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.04 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.04 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.48 us = 0.01% latency, 11.2 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.48 us = 0.01% latency, 11.2 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.96 us = 0.01% latency, 31.72 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.96 us = 0.01% latency, 31.72 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.84 ms = 0.22% latency, 84.47 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.3 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 324.96 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 324.96 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 202.66 us = 0.01% latency, 38.81 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 202.66 us = 0.01% latency, 38.81 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 253.2 us = 0.01% latency, 7.76 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 253.2 us = 0.01% latency, 7.76 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 250.82 us = 0.01% latency, 122.48 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 606.78 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.16 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 600.81 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.07 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (10): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.37 ms = 1.96% latency, 38.47 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.53 ms = 0.83% latency, 24 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.48 ms = 0.14% latency, 26.8 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 400.3 us = 0.02% latency, 157.17 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.38 us = 0.01% latency, 11.41 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.38 us = 0.01% latency, 11.41 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 242.95 us = 0.01% latency, 6.47 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 242.95 us = 0.01% latency, 6.47 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.96 ms = 0.11% latency, 17.5 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 252.49 us = 0.01% latency, 124.59 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 167.85 us = 0.01% latency, 11.71 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 167.85 us = 0.01% latency, 11.71 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.69 us = 0.01% latency, 5.05 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.69 us = 0.01% latency, 5.05 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.29 ms = 0.13% latency, 14.92 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 258.45 us = 0.01% latency, 121.72 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 169.28 us = 0.01% latency, 11.61 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 169.28 us = 0.01% latency, 11.61 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.65 us = 0.01% latency, 4.87 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.65 us = 0.01% latency, 4.87 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.33 ms = 0.25% latency, 15.36 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 344.99 us = 0.02% latency, 182.37 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 178.81 us = 0.01% latency, 8.8 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 178.81 us = 0.01% latency, 8.8 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 242.23 us = 0.01% latency, 8.12 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 242.23 us = 0.01% latency, 8.12 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 502.35 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.99 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.64 ms = 0.95% latency, 58.49 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.77 ms = 0.33% latency, 56.24 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 222.05 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.62 us = 0.01% latency, 11.39 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.62 us = 0.01% latency, 11.39 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 241.76 us = 0.01% latency, 32.53 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 241.76 us = 0.01% latency, 32.53 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.66 ms = 0.21% latency, 88.57 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.82 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 187.4 us = 0.01% latency, 10.49 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 187.4 us = 0.01% latency, 10.49 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 240.8 us = 0.01% latency, 32.66 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 240.8 us = 0.01% latency, 32.66 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.81 ms = 0.22% latency, 85.2 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.3 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 321.15 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 321.15 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 195.03 us = 0.01% latency, 40.32 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 195.03 us = 0.01% latency, 40.32 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 244.86 us = 0.01% latency, 8.03 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 244.86 us = 0.01% latency, 8.03 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 251.53 us = 0.01% latency, 122.13 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.07 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.59 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 599.62 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.35 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (11): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.4 ms = 1.97% latency, 38.43 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.49 ms = 0.83% latency, 24.06 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.49 ms = 0.14% latency, 26.71 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 401.26 us = 0.02% latency, 156.79 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 140.43 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 140.43 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175 us = 0.01% latency, 11.23 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175 us = 0.01% latency, 11.23 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 247.48 us = 0.01% latency, 6.36 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 247.48 us = 0.01% latency, 6.36 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.99 ms = 0.11% latency, 17.2 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 260.59 us = 0.01% latency, 120.71 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 169.28 us = 0.01% latency, 4.65 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 169.28 us = 0.01% latency, 4.65 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.21 ms = 0.13% latency, 15.46 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 254.39 us = 0.01% latency, 123.66 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 140.91 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 140.91 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.95 us = 0.01% latency, 11.17 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.95 us = 0.01% latency, 11.17 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 159.26 us = 0.01% latency, 4.94 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 159.26 us = 0.01% latency, 4.94 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.32 ms = 0.25% latency, 15.39 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 345.23 us = 0.02% latency, 182.24 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.41 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.41 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 182.63 us = 0.01% latency, 8.61 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 182.63 us = 0.01% latency, 8.61 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 247.72 us = 0.01% latency, 7.94 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 247.72 us = 0.01% latency, 7.94 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 502.11 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.75 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.72 ms = 0.96% latency, 58.22 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.76 ms = 0.33% latency, 56.28 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.65 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.48 us = 0.01% latency, 11.2 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.48 us = 0.01% latency, 11.2 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.52 us = 0.01% latency, 31.9 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.52 us = 0.01% latency, 31.9 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.68 ms = 0.21% latency, 88.14 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.76 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 253.44 us = 0.01% latency, 31.03 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 253.44 us = 0.01% latency, 31.03 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.84 ms = 0.22% latency, 84.45 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.39 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 324.96 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 324.96 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 197.17 us = 0.01% latency, 39.89 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 197.17 us = 0.01% latency, 39.89 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 253.44 us = 0.01% latency, 7.76 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 253.44 us = 0.01% latency, 7.76 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 252.72 us = 0.01% latency, 121.56 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 606.78 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.59 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 598.91 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.07 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (12): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 35.23 ms = 2.01% latency, 37.52 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 15.32 ms = 0.88% latency, 22.77 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 3.23 ms = 0.18% latency, 20.55 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 403.4 us = 0.02% latency, 155.96 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 138.76 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 138.76 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 179.29 us = 0.01% latency, 10.97 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 179.29 us = 0.01% latency, 10.97 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 258.92 us = 0.01% latency, 6.07 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 258.92 us = 0.01% latency, 6.07 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2 ms = 0.11% latency, 17.11 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 264.17 us = 0.02% latency, 119.08 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.46 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.46 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 185.01 us = 0.01% latency, 10.63 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 185.01 us = 0.01% latency, 10.63 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 162.36 us = 0.01% latency, 4.84 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 162.36 us = 0.01% latency, 4.84 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.24 ms = 0.13% latency, 15.28 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 265.6 us = 0.02% latency, 118.44 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.89 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.89 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.95 us = 0.01% latency, 11.17 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.95 us = 0.01% latency, 11.17 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 164.51 us = 0.01% latency, 4.78 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 164.51 us = 0.01% latency, 4.78 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.33 ms = 0.25% latency, 15.34 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 345.95 us = 0.02% latency, 181.86 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 175 us = 0.01% latency, 8.99 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 175 us = 0.01% latency, 8.99 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 257.25 us = 0.01% latency, 7.64 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 257.25 us = 0.01% latency, 7.64 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 503.06 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 271.56 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.72 ms = 0.96% latency, 58.21 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.77 ms = 0.33% latency, 56.27 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.57 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.46 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.46 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.57 us = 0.01% latency, 11.33 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.57 us = 0.01% latency, 11.33 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.34 us = 0.01% latency, 31.41 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.34 us = 0.01% latency, 31.41 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.67 ms = 0.21% latency, 88.4 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.29 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.09 us = 0.01% latency, 32.09 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.09 us = 0.01% latency, 32.09 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.85 ms = 0.22% latency, 84.19 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.05 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 327.59 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 327.59 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 201.94 us = 0.01% latency, 38.94 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 201.94 us = 0.01% latency, 38.94 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 242.95 us = 0.01% latency, 8.09 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 242.95 us = 0.01% latency, 8.09 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 255.58 us = 0.01% latency, 120.19 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.01 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 612.26 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 600.34 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.11 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (13): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.4 ms = 1.97% latency, 38.43 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.48 ms = 0.83% latency, 24.09 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.49 ms = 0.14% latency, 26.65 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 402.69 us = 0.02% latency, 156.24 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 178.81 us = 0.01% latency, 11 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 178.81 us = 0.01% latency, 11 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 249.39 us = 0.01% latency, 6.31 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 249.39 us = 0.01% latency, 6.31 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.97 ms = 0.11% latency, 17.41 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 257.73 us = 0.01% latency, 122.05 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.08 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.08 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.79 us = 0.01% latency, 4.95 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.79 us = 0.01% latency, 4.95 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.2 ms = 0.13% latency, 15.57 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 252.49 us = 0.01% latency, 124.59 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 179.05 us = 0.01% latency, 10.98 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 179.05 us = 0.01% latency, 10.98 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 156.88 us = 0.01% latency, 5.01 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 156.88 us = 0.01% latency, 5.01 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.34 ms = 0.25% latency, 15.3 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 346.66 us = 0.02% latency, 181.49 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.66 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.66 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 185.25 us = 0.01% latency, 8.49 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 185.25 us = 0.01% latency, 8.49 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 249.86 us = 0.01% latency, 7.87 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 249.86 us = 0.01% latency, 7.87 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 503.3 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 271.8 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.72 ms = 0.96% latency, 58.2 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.78 ms = 0.33% latency, 56.17 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.35 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.71 us = 0.01% latency, 11.19 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.71 us = 0.01% latency, 11.19 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.24 us = 0.01% latency, 31.81 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.24 us = 0.01% latency, 31.81 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.71 ms = 0.21% latency, 87.42 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.99 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.9 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.9 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 178.81 us = 0.01% latency, 11 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 178.81 us = 0.01% latency, 11 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 251.05 us = 0.01% latency, 31.33 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 251.05 us = 0.01% latency, 31.33 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.83 ms = 0.22% latency, 84.68 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.86 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 322.1 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 322.1 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 196.46 us = 0.01% latency, 40.03 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 196.46 us = 0.01% latency, 40.03 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 245.57 us = 0.01% latency, 8.01 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 245.57 us = 0.01% latency, 8.01 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 251.05 us = 0.01% latency, 122.36 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 607.25 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.31 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 598.67 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 613.45 us = 0.04% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (14): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.31 ms = 1.96% latency, 38.53 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.48 ms = 0.83% latency, 24.09 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.47 ms = 0.14% latency, 26.93 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 403.4 us = 0.02% latency, 155.96 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 247.96 us = 0.01% latency, 6.34 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 247.96 us = 0.01% latency, 6.34 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.01 ms = 0.11% latency, 17.05 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 262.5 us = 0.01% latency, 119.84 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 174.28 us = 0.01% latency, 11.28 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 174.28 us = 0.01% latency, 11.28 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 159.98 us = 0.01% latency, 4.92 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 159.98 us = 0.01% latency, 4.92 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.2 ms = 0.13% latency, 15.54 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 250.34 us = 0.01% latency, 125.66 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 137.33 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 137.33 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 177.38 us = 0.01% latency, 11.08 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 177.38 us = 0.01% latency, 11.08 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 165.94 us = 0.01% latency, 4.74 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 165.94 us = 0.01% latency, 4.74 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.31 ms = 0.25% latency, 15.42 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 345.47 us = 0.02% latency, 182.11 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 171.42 us = 0.01% latency, 9.18 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 171.42 us = 0.01% latency, 9.18 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 250.34 us = 0.01% latency, 7.85 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 250.34 us = 0.01% latency, 7.85 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 503.78 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 277.52 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.65 ms = 0.95% latency, 58.46 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.76 ms = 0.33% latency, 56.29 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.99 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.05 us = 0.01% latency, 31.96 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.05 us = 0.01% latency, 31.96 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.65 ms = 0.21% latency, 88.98 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.4 ms = 0.08% latency, 223.93 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 126.6 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 126.6 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 168.8 us = 0.01% latency, 11.65 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 168.8 us = 0.01% latency, 11.65 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.81 us = 0.01% latency, 31.99 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.81 us = 0.01% latency, 31.99 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.83 ms = 0.22% latency, 84.71 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 236.96 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 327.83 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 327.83 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 195.5 us = 0.01% latency, 40.23 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 195.5 us = 0.01% latency, 40.23 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 239.37 us = 0.01% latency, 8.21 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 239.37 us = 0.01% latency, 8.21 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 254.63 us = 0.01% latency, 120.65 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.01 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.64 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 600.58 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.16 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (15): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.37 ms = 1.96% latency, 38.47 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.42 ms = 0.82% latency, 24.19 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.45 ms = 0.14% latency, 27.13 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 402.45 us = 0.02% latency, 156.33 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 183.34 us = 0.01% latency, 10.72 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 183.34 us = 0.01% latency, 10.72 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 237.94 us = 0.01% latency, 6.61 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 237.94 us = 0.01% latency, 6.61 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.97 ms = 0.11% latency, 17.4 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 257.97 us = 0.01% latency, 121.94 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.27 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.27 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.66 us = 0.01% latency, 11.45 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.66 us = 0.01% latency, 11.45 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.31 us = 0.01% latency, 4.97 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.31 us = 0.01% latency, 4.97 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.19 ms = 0.13% latency, 15.62 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 249.62 us = 0.01% latency, 126.02 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 185.25 us = 0.01% latency, 10.61 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 185.25 us = 0.01% latency, 10.61 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 160.46 us = 0.01% latency, 4.9 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 160.46 us = 0.01% latency, 4.9 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.33 ms = 0.25% latency, 15.34 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 343.08 us = 0.02% latency, 183.38 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.66 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.66 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 175.24 us = 0.01% latency, 8.98 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 175.24 us = 0.01% latency, 8.98 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 246.52 us = 0.01% latency, 7.98 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 246.52 us = 0.01% latency, 7.98 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 502.82 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.04 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.76 ms = 0.96% latency, 58.07 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.77 ms = 0.33% latency, 56.27 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.54 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 132.08 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 132.08 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 179.77 us = 0.01% latency, 10.94 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 179.77 us = 0.01% latency, 10.94 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.05 us = 0.01% latency, 31.96 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.05 us = 0.01% latency, 31.96 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.74 ms = 0.21% latency, 86.84 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.06 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 136.61 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 136.61 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.47 us = 0.01% latency, 11.53 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.47 us = 0.01% latency, 11.53 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 243.43 us = 0.01% latency, 32.31 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 243.43 us = 0.01% latency, 32.31 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.85 ms = 0.22% latency, 84.35 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.52 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 329.97 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 329.97 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.32 us = 0.01% latency, 39.46 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.32 us = 0.01% latency, 39.46 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 240.8 us = 0.01% latency, 8.16 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 240.8 us = 0.01% latency, 8.16 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 249.86 us = 0.01% latency, 122.95 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 605.11 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 608.44 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 601.29 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 612.97 us = 0.04% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (16): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.36 ms = 1.96% latency, 38.48 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.46 ms = 0.83% latency, 24.12 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.48 ms = 0.14% latency, 26.8 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 401.26 us = 0.02% latency, 156.79 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.62 us = 0.01% latency, 11.39 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.62 us = 0.01% latency, 11.39 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 250.34 us = 0.01% latency, 6.28 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 250.34 us = 0.01% latency, 6.28 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.98 ms = 0.11% latency, 17.26 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 266.31 us = 0.02% latency, 118.12 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 179.29 us = 0.01% latency, 10.97 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 179.29 us = 0.01% latency, 10.97 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.31 us = 0.01% latency, 4.97 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.31 us = 0.01% latency, 4.97 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.18 ms = 0.12% latency, 15.68 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 249.86 us = 0.01% latency, 125.9 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 168.32 us = 0.01% latency, 11.68 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 168.32 us = 0.01% latency, 11.68 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.31 us = 0.01% latency, 4.97 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.31 us = 0.01% latency, 4.97 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.33 ms = 0.25% latency, 15.34 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 342.61 us = 0.02% latency, 183.63 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 141.14 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 141.14 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 174.76 us = 0.01% latency, 9 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 174.76 us = 0.01% latency, 9 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 250.34 us = 0.01% latency, 7.85 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 250.34 us = 0.01% latency, 7.85 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 501.87 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.51 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.72 ms = 0.96% latency, 58.22 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.77 ms = 0.33% latency, 56.21 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.33 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 174.52 us = 0.01% latency, 11.27 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 174.52 us = 0.01% latency, 11.27 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 258.21 us = 0.01% latency, 30.46 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 258.21 us = 0.01% latency, 30.46 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.67 ms = 0.21% latency, 88.44 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.33 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.24 us = 0.01% latency, 31.81 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.24 us = 0.01% latency, 31.81 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.85 ms = 0.22% latency, 84.23 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 236.79 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 321.15 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 321.15 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 198.84 us = 0.01% latency, 39.55 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 198.84 us = 0.01% latency, 39.55 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 248.67 us = 0.01% latency, 7.91 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 248.67 us = 0.01% latency, 7.91 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 250.1 us = 0.01% latency, 122.83 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 605.82 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.4 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 599.62 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.83 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (17): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.53 ms = 1.97% latency, 38.29 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.51 ms = 0.83% latency, 24.03 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.52 ms = 0.14% latency, 26.39 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 401.02 us = 0.02% latency, 156.89 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.18 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.18 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 188.35 us = 0.01% latency, 10.44 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 188.35 us = 0.01% latency, 10.44 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 247.96 us = 0.01% latency, 6.34 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 247.96 us = 0.01% latency, 6.34 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.01 ms = 0.11% latency, 17.01 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 259.16 us = 0.01% latency, 121.38 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.24 us = 0.01% latency, 11.22 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.24 us = 0.01% latency, 11.22 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 163.56 us = 0.01% latency, 4.81 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 163.56 us = 0.01% latency, 4.81 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.18 ms = 0.12% latency, 15.73 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 253.44 us = 0.01% latency, 124.12 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 169.99 us = 0.01% latency, 11.57 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 169.99 us = 0.01% latency, 11.57 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.97 us = 0.01% latency, 5.07 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.97 us = 0.01% latency, 5.07 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.3 ms = 0.25% latency, 15.44 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 344.75 us = 0.02% latency, 182.49 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 172.38 us = 0.01% latency, 9.12 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 172.38 us = 0.01% latency, 9.12 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 249.39 us = 0.01% latency, 7.88 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 249.39 us = 0.01% latency, 7.88 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 502.59 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 275.61 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.82 ms = 0.96% latency, 57.85 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.86 ms = 0.33% latency, 55.36 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.14 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.33 us = 0.01% latency, 11.34 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.33 us = 0.01% latency, 11.34 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 249.15 us = 0.01% latency, 31.56 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 249.15 us = 0.01% latency, 31.56 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.68 ms = 0.21% latency, 88.04 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.67 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.75 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.75 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 241.99 us = 0.01% latency, 32.5 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 241.99 us = 0.01% latency, 32.5 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.87 ms = 0.22% latency, 83.84 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.69 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 325.92 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 325.92 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 200.75 us = 0.01% latency, 39.17 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 200.75 us = 0.01% latency, 39.17 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 249.86 us = 0.01% latency, 7.87 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 249.86 us = 0.01% latency, 7.87 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 252.72 us = 0.01% latency, 121.56 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 603.68 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 612.5 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 598.91 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 608.92 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (18): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.34 ms = 1.96% latency, 38.5 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.44 ms = 0.83% latency, 24.15 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.47 ms = 0.14% latency, 26.87 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 402.93 us = 0.02% latency, 156.14 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 244.14 us = 0.01% latency, 6.44 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 244.14 us = 0.01% latency, 6.44 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.98 ms = 0.11% latency, 17.31 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 263.69 us = 0.02% latency, 119.3 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.94 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.94 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.57 us = 0.01% latency, 11.33 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.57 us = 0.01% latency, 11.33 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.55 us = 0.01% latency, 4.96 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.55 us = 0.01% latency, 4.96 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.18 ms = 0.12% latency, 15.66 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 251.05 us = 0.01% latency, 125.3 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 126.84 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 126.84 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 168.32 us = 0.01% latency, 11.68 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 168.32 us = 0.01% latency, 11.68 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.65 us = 0.01% latency, 4.87 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.65 us = 0.01% latency, 4.87 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.3 ms = 0.25% latency, 15.46 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 344.28 us = 0.02% latency, 182.74 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.75 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.75 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 174.05 us = 0.01% latency, 9.04 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 174.05 us = 0.01% latency, 9.04 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 251.53 us = 0.01% latency, 7.82 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 251.53 us = 0.01% latency, 7.82 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 502.35 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.04 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.71 ms = 0.95% latency, 58.24 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.77 ms = 0.33% latency, 56.26 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.84 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.89 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.89 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 169.99 us = 0.01% latency, 11.57 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 169.99 us = 0.01% latency, 11.57 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 251.29 us = 0.01% latency, 31.3 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 251.29 us = 0.01% latency, 31.3 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.67 ms = 0.21% latency, 88.5 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.59 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.27 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.27 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.62 us = 0.01% latency, 11.39 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.62 us = 0.01% latency, 11.39 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.23 us = 0.01% latency, 32.47 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.23 us = 0.01% latency, 32.47 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.85 ms = 0.22% latency, 84.31 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.95 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 327.35 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 327.35 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.56 us = 0.01% latency, 39.41 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.56 us = 0.01% latency, 39.41 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 245.81 us = 0.01% latency, 8 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 245.81 us = 0.01% latency, 8 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 256.78 us = 0.01% latency, 119.64 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.48 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 608.21 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 600.1 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.83 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (19): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.3 ms = 1.96% latency, 38.54 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.42 ms = 0.82% latency, 24.18 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.47 ms = 0.14% latency, 26.87 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 401.97 us = 0.02% latency, 156.51 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.09 us = 0.01% latency, 11.36 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.09 us = 0.01% latency, 11.36 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 241.52 us = 0.01% latency, 6.51 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 241.52 us = 0.01% latency, 6.51 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.96 ms = 0.11% latency, 17.47 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 254.39 us = 0.01% latency, 123.66 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.51 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.51 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 168.8 us = 0.01% latency, 11.65 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 168.8 us = 0.01% latency, 11.65 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 160.22 us = 0.01% latency, 4.91 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 160.22 us = 0.01% latency, 4.91 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.18 ms = 0.12% latency, 15.7 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 250.34 us = 0.01% latency, 125.66 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.03 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.03 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 174.28 us = 0.01% latency, 11.28 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 174.28 us = 0.01% latency, 11.28 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.69 us = 0.01% latency, 5.05 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.69 us = 0.01% latency, 5.05 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.33 ms = 0.25% latency, 15.36 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 344.51 us = 0.02% latency, 182.62 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 173.81 us = 0.01% latency, 9.05 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 173.81 us = 0.01% latency, 9.05 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 246.52 us = 0.01% latency, 7.98 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 246.52 us = 0.01% latency, 7.98 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 501.87 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 273.94 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.69 ms = 0.95% latency, 58.3 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.76 ms = 0.33% latency, 56.31 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.84 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.03 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.03 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 178.34 us = 0.01% latency, 11.02 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 178.34 us = 0.01% latency, 11.02 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.29 us = 0.01% latency, 31.93 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.29 us = 0.01% latency, 31.93 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.66 ms = 0.21% latency, 88.66 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.42 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.9 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.9 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 168.56 us = 0.01% latency, 11.66 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 168.56 us = 0.01% latency, 11.66 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.23 us = 0.01% latency, 32.47 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.23 us = 0.01% latency, 32.47 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.84 ms = 0.22% latency, 84.4 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.26 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 325.68 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 325.68 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 203.85 us = 0.01% latency, 38.58 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 203.85 us = 0.01% latency, 38.58 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 245.09 us = 0.01% latency, 8.02 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 245.09 us = 0.01% latency, 8.02 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 253.44 us = 0.01% latency, 121.21 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.72 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.87 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 601.05 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 612.02 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (20): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.27 ms = 1.96% latency, 38.57 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.45 ms = 0.83% latency, 24.14 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.48 ms = 0.14% latency, 26.78 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 400.3 us = 0.02% latency, 157.17 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 174.05 us = 0.01% latency, 11.3 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 174.05 us = 0.01% latency, 11.3 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 255.35 us = 0.01% latency, 6.16 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 255.35 us = 0.01% latency, 6.16 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.95 ms = 0.11% latency, 17.5 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 258.92 us = 0.01% latency, 121.49 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.95 us = 0.01% latency, 11.17 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.95 us = 0.01% latency, 11.17 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.83 us = 0.01% latency, 4.98 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.83 us = 0.01% latency, 4.98 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.19 ms = 0.13% latency, 15.6 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 258.45 us = 0.01% latency, 121.72 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.94 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.94 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 169.99 us = 0.01% latency, 11.57 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 169.99 us = 0.01% latency, 11.57 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.59 us = 0.01% latency, 4.99 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.59 us = 0.01% latency, 4.99 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.34 ms = 0.25% latency, 15.3 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 344.04 us = 0.02% latency, 182.87 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 137.57 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 137.57 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 185.97 us = 0.01% latency, 8.46 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 185.97 us = 0.01% latency, 8.46 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 244.86 us = 0.01% latency, 8.03 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 244.86 us = 0.01% latency, 8.03 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 502.35 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.04 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.64 ms = 0.95% latency, 58.49 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.75 ms = 0.33% latency, 56.37 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.42 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 253.44 us = 0.01% latency, 31.03 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 253.44 us = 0.01% latency, 31.03 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.67 ms = 0.21% latency, 88.35 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.48 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.33 us = 0.01% latency, 11.34 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.33 us = 0.01% latency, 11.34 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.23 us = 0.01% latency, 32.47 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.23 us = 0.01% latency, 32.47 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.81 ms = 0.22% latency, 85.17 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.6 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 330.45 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 330.45 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 194.79 us = 0.01% latency, 40.37 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 194.79 us = 0.01% latency, 40.37 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 243.43 us = 0.01% latency, 8.08 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 243.43 us = 0.01% latency, 8.08 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 248.91 us = 0.01% latency, 123.42 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 603.68 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.11 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 599.15 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.11 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (21): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.35 ms = 1.96% latency, 38.48 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.54 ms = 0.83% latency, 23.99 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.47 ms = 0.14% latency, 26.9 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 402.69 us = 0.02% latency, 156.24 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.08 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.08 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 181.2 us = 0.01% latency, 10.85 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 181.2 us = 0.01% latency, 10.85 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 246.52 us = 0.01% latency, 6.38 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 246.52 us = 0.01% latency, 6.38 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.03 ms = 0.12% latency, 16.83 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 253.92 us = 0.01% latency, 123.89 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.27 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.27 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 166.65 us = 0.01% latency, 11.8 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 166.65 us = 0.01% latency, 11.8 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.97 us = 0.01% latency, 5.07 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.97 us = 0.01% latency, 5.07 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.21 ms = 0.13% latency, 15.47 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 246.52 us = 0.01% latency, 127.6 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.41 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.41 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 186.44 us = 0.01% latency, 10.55 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 186.44 us = 0.01% latency, 10.55 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 162.6 us = 0.01% latency, 4.84 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 162.6 us = 0.01% latency, 4.84 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.34 ms = 0.25% latency, 15.33 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 344.51 us = 0.02% latency, 182.62 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 176.91 us = 0.01% latency, 8.89 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 176.91 us = 0.01% latency, 8.89 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 241.76 us = 0.01% latency, 8.13 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 241.76 us = 0.01% latency, 8.13 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 501.39 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.51 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.62 ms = 0.95% latency, 58.54 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.79 ms = 0.33% latency, 56.07 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.54 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 178.34 us = 0.01% latency, 11.02 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 178.34 us = 0.01% latency, 11.02 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.1 us = 0.01% latency, 31.44 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.1 us = 0.01% latency, 31.44 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.63 ms = 0.21% latency, 89.3 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.44 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 124.45 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 124.45 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 169.75 us = 0.01% latency, 11.58 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 169.75 us = 0.01% latency, 11.58 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.95 us = 0.01% latency, 32.37 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.95 us = 0.01% latency, 32.37 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.81 ms = 0.22% latency, 85.14 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 236.79 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 323.3 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 323.3 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 197.89 us = 0.01% latency, 39.74 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 197.89 us = 0.01% latency, 39.74 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 235.8 us = 0.01% latency, 8.34 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 235.8 us = 0.01% latency, 8.34 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 249.62 us = 0.01% latency, 123.06 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 603.44 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 608.92 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 598.19 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.83 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (22): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.19 ms = 1.95% latency, 38.66 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.37 ms = 0.82% latency, 24.27 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.47 ms = 0.14% latency, 26.88 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 411.03 us = 0.02% latency, 153.06 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.09 us = 0.01% latency, 11.36 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.09 us = 0.01% latency, 11.36 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 250.58 us = 0.01% latency, 6.28 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 250.58 us = 0.01% latency, 6.28 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.97 ms = 0.11% latency, 17.4 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 253.92 us = 0.01% latency, 123.89 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.62 us = 0.01% latency, 11.39 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.62 us = 0.01% latency, 11.39 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 156.64 us = 0.01% latency, 5.02 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 156.64 us = 0.01% latency, 5.02 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.17 ms = 0.12% latency, 15.74 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 253.44 us = 0.01% latency, 124.12 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 168.8 us = 0.01% latency, 11.65 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 168.8 us = 0.01% latency, 11.65 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.07 us = 0.01% latency, 4.98 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.07 us = 0.01% latency, 4.98 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.29 ms = 0.24% latency, 15.51 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 341.65 us = 0.02% latency, 184.15 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 178.34 us = 0.01% latency, 8.82 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 178.34 us = 0.01% latency, 8.82 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 243.19 us = 0.01% latency, 8.08 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 243.19 us = 0.01% latency, 8.08 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 499.96 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.04 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.63 ms = 0.95% latency, 58.53 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.74 ms = 0.33% latency, 56.47 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.61 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 132.32 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 132.32 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.23 us = 0.01% latency, 11.55 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.23 us = 0.01% latency, 11.55 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 244.38 us = 0.01% latency, 32.18 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 244.38 us = 0.01% latency, 32.18 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.67 ms = 0.21% latency, 88.3 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.84 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 169.04 us = 0.01% latency, 11.63 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 169.04 us = 0.01% latency, 11.63 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 252.25 us = 0.01% latency, 31.18 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 252.25 us = 0.01% latency, 31.18 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.82 ms = 0.22% latency, 84.99 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.35 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 327.83 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 327.83 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 196.46 us = 0.01% latency, 40.03 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 196.46 us = 0.01% latency, 40.03 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 239.61 us = 0.01% latency, 8.21 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 239.61 us = 0.01% latency, 8.21 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 249.39 us = 0.01% latency, 123.18 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 606.06 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 612.5 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 599.86 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.54 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (23): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.41 ms = 1.97% latency, 38.42 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.51 ms = 0.83% latency, 24.03 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.57 ms = 0.15% latency, 25.86 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 404.83 us = 0.02% latency, 155.41 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.09 us = 0.01% latency, 11.36 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.09 us = 0.01% latency, 11.36 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 244.14 us = 0.01% latency, 6.44 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 244.14 us = 0.01% latency, 6.44 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.96 ms = 0.11% latency, 17.5 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 259.88 us = 0.01% latency, 121.05 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 167.37 us = 0.01% latency, 11.75 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 167.37 us = 0.01% latency, 11.75 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 153.06 us = 0.01% latency, 5.14 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 153.06 us = 0.01% latency, 5.14 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.19 ms = 0.13% latency, 15.64 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 251.29 us = 0.01% latency, 125.18 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.9 us = 0.01% latency, 11.44 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 162.84 us = 0.01% latency, 4.83 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 162.84 us = 0.01% latency, 4.83 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.29 ms = 0.24% latency, 15.5 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 343.56 us = 0.02% latency, 183.12 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 172.62 us = 0.01% latency, 9.11 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 172.62 us = 0.01% latency, 9.11 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 250.58 us = 0.01% latency, 7.85 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 250.58 us = 0.01% latency, 7.85 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 500.44 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.99 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.71 ms = 0.96% latency, 58.23 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.77 ms = 0.33% latency, 56.19 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 222.24 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 126.84 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 126.84 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.38 us = 0.01% latency, 11.41 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.38 us = 0.01% latency, 11.41 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 249.62 us = 0.01% latency, 31.5 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 249.62 us = 0.01% latency, 31.5 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.65 ms = 0.21% latency, 88.86 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.78 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.08 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.08 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 169.75 us = 0.01% latency, 11.58 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 169.75 us = 0.01% latency, 11.58 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.95 us = 0.01% latency, 32.37 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.95 us = 0.01% latency, 32.37 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.85 ms = 0.22% latency, 84.28 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.39 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 325.92 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 325.92 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 198.84 us = 0.01% latency, 39.55 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 198.84 us = 0.01% latency, 39.55 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 246.29 us = 0.01% latency, 7.98 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 246.29 us = 0.01% latency, 7.98 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 251.53 us = 0.01% latency, 122.13 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 600.81 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.11 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 599.38 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 608.68 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (24): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.31 ms = 1.96% latency, 38.53 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.34 ms = 0.82% latency, 24.33 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.47 ms = 0.14% latency, 26.92 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 403.17 us = 0.02% latency, 156.05 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 136.61 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 136.61 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.23 us = 0.01% latency, 11.55 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.23 us = 0.01% latency, 11.55 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 239.37 us = 0.01% latency, 6.57 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 239.37 us = 0.01% latency, 6.57 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.93 ms = 0.11% latency, 17.71 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 250.58 us = 0.01% latency, 125.54 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 126.6 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 126.6 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 168.32 us = 0.01% latency, 11.68 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 168.32 us = 0.01% latency, 11.68 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.02 us = 0.01% latency, 5.11 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.02 us = 0.01% latency, 5.11 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.18 ms = 0.12% latency, 15.68 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 255.58 us = 0.01% latency, 123.08 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 126.12 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 126.12 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.47 us = 0.01% latency, 11.53 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.47 us = 0.01% latency, 11.53 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.21 us = 0.01% latency, 5.07 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.21 us = 0.01% latency, 5.07 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.29 ms = 0.25% latency, 15.48 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 343.32 us = 0.02% latency, 183.25 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.46 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.46 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 173.09 us = 0.01% latency, 9.09 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 173.09 us = 0.01% latency, 9.09 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 243.9 us = 0.01% latency, 8.06 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 243.9 us = 0.01% latency, 8.06 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 500.2 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.27 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.8 ms = 0.96% latency, 57.93 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.75 ms = 0.33% latency, 56.41 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.57 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.32 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.32 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 177.62 us = 0.01% latency, 11.07 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 177.62 us = 0.01% latency, 11.07 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.09 us = 0.01% latency, 32.09 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.09 us = 0.01% latency, 32.09 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.78 ms = 0.22% latency, 85.75 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.4 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 142.1 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 142.1 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 177.86 us = 0.01% latency, 11.05 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 177.86 us = 0.01% latency, 11.05 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.82 us = 0.01% latency, 31.35 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.82 us = 0.01% latency, 31.35 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.86 ms = 0.22% latency, 84.06 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.6 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 329.97 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 329.97 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 197.89 us = 0.01% latency, 39.74 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 197.89 us = 0.01% latency, 39.74 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 246.05 us = 0.01% latency, 7.99 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 246.05 us = 0.01% latency, 7.99 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 250.58 us = 0.01% latency, 122.6 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 603.44 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 607.01 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 598.67 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 612.02 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (25): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.35 ms = 1.96% latency, 38.49 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.44 ms = 0.83% latency, 24.15 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.49 ms = 0.14% latency, 26.72 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 401.26 us = 0.02% latency, 156.79 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.75 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.75 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.48 us = 0.01% latency, 11.2 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.48 us = 0.01% latency, 11.2 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 251.05 us = 0.01% latency, 6.27 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 251.05 us = 0.01% latency, 6.27 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.97 ms = 0.11% latency, 17.35 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 262.98 us = 0.02% latency, 119.62 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 125.41 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 125.41 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 160.69 us = 0.01% latency, 4.89 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 160.69 us = 0.01% latency, 4.89 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.19 ms = 0.12% latency, 15.65 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 254.15 us = 0.01% latency, 123.77 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.51 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.51 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 169.52 us = 0.01% latency, 11.6 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 169.52 us = 0.01% latency, 11.6 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.97 us = 0.01% latency, 5.07 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.97 us = 0.01% latency, 5.07 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.32 ms = 0.25% latency, 15.39 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 343.56 us = 0.02% latency, 183.12 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.27 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.27 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 172.38 us = 0.01% latency, 9.12 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 172.38 us = 0.01% latency, 9.12 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 257.73 us = 0.01% latency, 7.63 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 257.73 us = 0.01% latency, 7.63 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 503.3 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 271.56 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.72 ms = 0.96% latency, 58.19 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.76 ms = 0.33% latency, 56.3 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.1 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.33 us = 0.01% latency, 11.34 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.33 us = 0.01% latency, 11.34 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 254.87 us = 0.01% latency, 30.86 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 254.87 us = 0.01% latency, 30.86 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.68 ms = 0.21% latency, 88.16 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.67 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.18 us = 0.01% latency, 11.49 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.18 us = 0.01% latency, 11.49 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247 us = 0.01% latency, 31.84 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247 us = 0.01% latency, 31.84 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.86 ms = 0.22% latency, 84.09 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.3 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 326.16 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 326.16 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 198.13 us = 0.01% latency, 39.69 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 198.13 us = 0.01% latency, 39.69 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 246.29 us = 0.01% latency, 7.98 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 246.29 us = 0.01% latency, 7.98 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 251.53 us = 0.01% latency, 122.13 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.48 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.54 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 599.15 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.64 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (26): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 35.49 ms = 2.03% latency, 37.25 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 15.57 ms = 0.89% latency, 22.4 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 3.18 ms = 0.18% latency, 20.9 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 401.74 us = 0.02% latency, 156.61 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 215.53 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 215.53 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 259.88 us = 0.01% latency, 7.57 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 259.88 us = 0.01% latency, 7.57 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 289.2 us = 0.02% latency, 5.44 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 289.2 us = 0.02% latency, 5.44 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.34 ms = 0.13% latency, 14.65 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 272.51 us = 0.02% latency, 115.43 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 160.93 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 160.93 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 197.65 us = 0.01% latency, 9.95 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 197.65 us = 0.01% latency, 9.95 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 162.12 us = 0.01% latency, 4.85 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 162.12 us = 0.01% latency, 4.85 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.22 ms = 0.13% latency, 15.42 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 260.35 us = 0.01% latency, 120.83 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.85 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.73 us = 0.01% latency, 5.08 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.73 us = 0.01% latency, 5.08 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.32 ms = 0.25% latency, 15.39 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 346.18 us = 0.02% latency, 181.74 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.13 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 182.15 us = 0.01% latency, 8.63 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 182.15 us = 0.01% latency, 8.63 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 247.48 us = 0.01% latency, 7.94 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 247.48 us = 0.01% latency, 7.94 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 506.64 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.75 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.72 ms = 0.96% latency, 58.22 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.9 ms = 0.34% latency, 54.97 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.91 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 139 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 139 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 179.77 us = 0.01% latency, 10.94 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 179.77 us = 0.01% latency, 10.94 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 252.25 us = 0.01% latency, 31.18 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 252.25 us = 0.01% latency, 31.18 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.67 ms = 0.21% latency, 88.42 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.91 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.94 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.94 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.85 us = 0.01% latency, 11.37 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.85 us = 0.01% latency, 11.37 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 252.96 us = 0.01% latency, 31.09 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 252.96 us = 0.01% latency, 31.09 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.83 ms = 0.22% latency, 84.62 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.05 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 324.01 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 324.01 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 200.27 us = 0.01% latency, 39.27 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 200.27 us = 0.01% latency, 39.27 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 243.19 us = 0.01% latency, 8.08 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 243.19 us = 0.01% latency, 8.08 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 255.58 us = 0.01% latency, 120.19 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 604.15 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.78 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 597.72 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.31 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (27): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.35 ms = 1.96% latency, 38.49 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.44 ms = 0.83% latency, 24.15 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.49 ms = 0.14% latency, 26.74 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 405.31 us = 0.02% latency, 155.23 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.51 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.51 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.18 us = 0.01% latency, 11.49 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.18 us = 0.01% latency, 11.49 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 249.62 us = 0.01% latency, 6.3 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 249.62 us = 0.01% latency, 6.3 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.97 ms = 0.11% latency, 17.39 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 262.74 us = 0.02% latency, 119.73 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.75 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.75 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 169.75 us = 0.01% latency, 11.58 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 169.75 us = 0.01% latency, 11.58 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.02 us = 0.01% latency, 5.11 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.02 us = 0.01% latency, 5.11 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.17 ms = 0.12% latency, 15.75 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 249.62 us = 0.01% latency, 126.02 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 174.05 us = 0.01% latency, 11.3 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 174.05 us = 0.01% latency, 11.3 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.17 us = 0.01% latency, 4.88 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 161.17 us = 0.01% latency, 4.88 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.32 ms = 0.25% latency, 15.39 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 345.71 us = 0.02% latency, 181.99 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 173.81 us = 0.01% latency, 9.05 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 173.81 us = 0.01% latency, 9.05 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 252.25 us = 0.01% latency, 7.79 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 252.25 us = 0.01% latency, 7.79 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 503.06 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 278.71 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.72 ms = 0.96% latency, 58.22 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.77 ms = 0.33% latency, 56.26 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.54 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.51 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.51 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.33 us = 0.01% latency, 11.34 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.33 us = 0.01% latency, 11.34 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.24 us = 0.01% latency, 31.81 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.24 us = 0.01% latency, 31.81 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.68 ms = 0.21% latency, 88.14 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.4 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.65 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.65 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.14 us = 0.01% latency, 11.42 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.14 us = 0.01% latency, 11.42 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.52 us = 0.01% latency, 31.9 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 246.52 us = 0.01% latency, 31.9 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.85 ms = 0.22% latency, 84.33 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.05 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 326.63 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 326.63 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 200.27 us = 0.01% latency, 39.27 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 200.27 us = 0.01% latency, 39.27 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 247.48 us = 0.01% latency, 7.94 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 247.48 us = 0.01% latency, 7.94 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 255.82 us = 0.01% latency, 120.08 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 603.2 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.4 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 600.34 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.83 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (28): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.23 ms = 1.96% latency, 38.62 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.41 ms = 0.82% latency, 24.21 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.48 ms = 0.14% latency, 26.81 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 404.83 us = 0.02% latency, 155.41 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.89 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.89 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.47 us = 0.01% latency, 11.53 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.47 us = 0.01% latency, 11.53 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 242.95 us = 0.01% latency, 6.47 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 242.95 us = 0.01% latency, 6.47 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.94 ms = 0.11% latency, 17.66 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 253.92 us = 0.01% latency, 123.89 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 123.26 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 123.26 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 168.8 us = 0.01% latency, 11.65 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 168.8 us = 0.01% latency, 11.65 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.12 us = 0.01% latency, 5.01 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.12 us = 0.01% latency, 5.01 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.19 ms = 0.13% latency, 15.6 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 255.58 us = 0.01% latency, 123.08 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.85 us = 0.01% latency, 11.37 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.85 us = 0.01% latency, 11.37 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 156.88 us = 0.01% latency, 5.01 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 156.88 us = 0.01% latency, 5.01 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.3 ms = 0.25% latency, 15.46 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 344.04 us = 0.02% latency, 182.87 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 173.81 us = 0.01% latency, 9.05 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 173.81 us = 0.01% latency, 9.05 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 247.72 us = 0.01% latency, 7.94 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 247.72 us = 0.01% latency, 7.94 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 503.78 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 273.47 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.64 ms = 0.95% latency, 58.49 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.74 ms = 0.33% latency, 56.5 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.91 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 126.12 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 126.12 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 177.38 us = 0.01% latency, 11.08 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 177.38 us = 0.01% latency, 11.08 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.95 us = 0.01% latency, 32.37 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 242.95 us = 0.01% latency, 32.37 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.65 ms = 0.21% latency, 88.88 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.9 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.18 us = 0.01% latency, 11.49 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.18 us = 0.01% latency, 11.49 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 240.33 us = 0.01% latency, 32.72 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 240.33 us = 0.01% latency, 32.72 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.83 ms = 0.22% latency, 84.68 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.43 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 324.25 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 324.25 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 202.66 us = 0.01% latency, 38.81 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 202.66 us = 0.01% latency, 38.81 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 245.09 us = 0.01% latency, 8.02 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 245.09 us = 0.01% latency, 8.02 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 251.05 us = 0.01% latency, 122.36 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 604.39 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.35 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 600.1 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.83 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (29): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.29 ms = 1.96% latency, 38.55 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.42 ms = 0.82% latency, 24.18 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.49 ms = 0.14% latency, 26.69 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 402.93 us = 0.02% latency, 156.14 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 134.23 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 134.23 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175 us = 0.01% latency, 11.23 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175 us = 0.01% latency, 11.23 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 257.49 us = 0.01% latency, 6.11 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 257.49 us = 0.01% latency, 6.11 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.96 ms = 0.11% latency, 17.45 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 266.08 us = 0.02% latency, 118.23 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 126.36 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 126.36 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.95 us = 0.01% latency, 11.17 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.95 us = 0.01% latency, 11.17 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.55 us = 0.01% latency, 4.96 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 158.55 us = 0.01% latency, 4.96 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.18 ms = 0.12% latency, 15.68 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 256.54 us = 0.01% latency, 122.62 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.46 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.46 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.47 us = 0.01% latency, 11.53 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.47 us = 0.01% latency, 11.53 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.02 us = 0.01% latency, 5.11 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.02 us = 0.01% latency, 5.11 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.31 ms = 0.25% latency, 15.43 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 347.85 us = 0.02% latency, 180.87 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.42 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.42 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 173.33 us = 0.01% latency, 9.07 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 173.33 us = 0.01% latency, 9.07 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 243.9 us = 0.01% latency, 8.06 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 243.9 us = 0.01% latency, 8.06 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 502.82 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.27 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.67 ms = 0.95% latency, 58.38 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.77 ms = 0.33% latency, 56.18 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.36 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.65 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.65 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.38 us = 0.01% latency, 11.41 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.38 us = 0.01% latency, 11.41 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 254.15 us = 0.01% latency, 30.94 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 254.15 us = 0.01% latency, 30.94 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.68 ms = 0.21% latency, 88.05 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.36 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 175.71 us = 0.01% latency, 11.19 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 175.71 us = 0.01% latency, 11.19 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.34 us = 0.01% latency, 31.41 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 250.34 us = 0.01% latency, 31.41 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.81 ms = 0.22% latency, 85.18 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.69 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 324.73 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 324.73 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 198.13 us = 0.01% latency, 39.69 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 198.13 us = 0.01% latency, 39.69 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 247.72 us = 0.01% latency, 7.94 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 247.72 us = 0.01% latency, 7.94 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 249.86 us = 0.01% latency, 122.95 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 606.54 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.54 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 600.81 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.87 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (30): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.37 ms = 1.96% latency, 38.46 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.56 ms = 0.83% latency, 23.96 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.5 ms = 0.14% latency, 26.63 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 403.88 us = 0.02% latency, 155.77 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.55 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 173.57 us = 0.01% latency, 11.33 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 173.57 us = 0.01% latency, 11.33 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 252.72 us = 0.01% latency, 6.22 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 252.72 us = 0.01% latency, 6.22 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.08 ms = 0.12% latency, 16.48 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 259.88 us = 0.01% latency, 121.05 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.18 us = 0.01% latency, 11.49 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.18 us = 0.01% latency, 11.49 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.5 us = 0.01% latency, 5.09 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 154.5 us = 0.01% latency, 5.09 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.19 ms = 0.13% latency, 15.61 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 248.43 us = 0.01% latency, 126.62 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.22 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 180.01 us = 0.01% latency, 10.92 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 180.01 us = 0.01% latency, 10.92 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.12 us = 0.01% latency, 5.01 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.12 us = 0.01% latency, 5.01 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.32 ms = 0.25% latency, 15.4 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 345.71 us = 0.02% latency, 181.99 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 131.37 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 171.18 us = 0.01% latency, 9.19 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 171.18 us = 0.01% latency, 9.19 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 246.52 us = 0.01% latency, 7.98 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 246.52 us = 0.01% latency, 7.98 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 503.78 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.51 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.63 ms = 0.95% latency, 58.52 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.77 ms = 0.33% latency, 56.26 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.25 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 178.81 us = 0.01% latency, 11 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 178.81 us = 0.01% latency, 11 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.57 us = 0.01% latency, 32.02 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.57 us = 0.01% latency, 32.02 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.64 ms = 0.21% latency, 89.08 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.9 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 126.6 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 126.6 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 169.04 us = 0.01% latency, 11.63 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 169.04 us = 0.01% latency, 11.63 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 244.62 us = 0.01% latency, 32.15 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 244.62 us = 0.01% latency, 32.15 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.81 ms = 0.22% latency, 85.19 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.9 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 325.44 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 325.44 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 195.26 us = 0.01% latency, 40.28 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 195.26 us = 0.01% latency, 40.28 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 238.66 us = 0.01% latency, 8.24 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 238.66 us = 0.01% latency, 8.24 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 250.82 us = 0.01% latency, 122.48 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 604.15 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.11 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 599.86 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 611.07 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (31): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.26 ms = 1.96% latency, 38.59 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.42 ms = 0.82% latency, 24.19 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.47 ms = 0.14% latency, 26.91 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 409.84 us = 0.02% latency, 153.51 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.65 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.65 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.47 us = 0.01% latency, 11.53 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.47 us = 0.01% latency, 11.53 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 246.52 us = 0.01% latency, 6.38 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 246.52 us = 0.01% latency, 6.38 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.97 ms = 0.11% latency, 17.35 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 256.06 us = 0.01% latency, 122.85 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.03 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.03 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.71 us = 0.01% latency, 11.52 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.71 us = 0.01% latency, 11.52 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.59 us = 0.01% latency, 4.99 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.59 us = 0.01% latency, 4.99 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.17 ms = 0.12% latency, 15.77 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 257.02 us = 0.01% latency, 122.39 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.94 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.94 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.45 us = 0.01% latency, 5.06 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.45 us = 0.01% latency, 5.06 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.32 ms = 0.25% latency, 15.38 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 340.22 us = 0.02% latency, 184.92 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 134.94 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 134.94 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 186.2 us = 0.01% latency, 8.45 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 186.2 us = 0.01% latency, 8.45 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 245.81 us = 0.01% latency, 8 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 245.81 us = 0.01% latency, 8 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 503.54 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 273.23 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.66 ms = 0.95% latency, 58.41 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.76 ms = 0.33% latency, 56.33 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.42 ms = 0.08% latency, 222.27 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 134.94 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 134.94 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 174.28 us = 0.01% latency, 11.28 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 174.28 us = 0.01% latency, 11.28 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 248.19 us = 0.01% latency, 31.69 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 248.19 us = 0.01% latency, 31.69 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.65 ms = 0.21% latency, 88.93 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.59 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 125.17 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 125.17 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 169.04 us = 0.01% latency, 11.63 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 169.04 us = 0.01% latency, 11.63 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.33 us = 0.01% latency, 32.06 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.33 us = 0.01% latency, 32.06 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.84 ms = 0.22% latency, 84.41 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 237.39 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 321.87 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 321.87 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.79 us = 0.01% latency, 39.36 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.79 us = 0.01% latency, 39.36 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 242.23 us = 0.01% latency, 8.12 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 242.23 us = 0.01% latency, 8.12 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 251.29 us = 0.01% latency, 122.25 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.01 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.87 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 601.29 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 613.69 us = 0.04% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (32): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 34.34 ms = 1.96% latency, 38.5 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.48 ms = 0.83% latency, 24.08 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.54 ms = 0.15% latency, 26.11 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 405.79 us = 0.02% latency, 155.04 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.66 us = 0.01% latency, 11.45 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.66 us = 0.01% latency, 11.45 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 243.66 us = 0.01% latency, 6.46 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 243.66 us = 0.01% latency, 6.46 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.96 ms = 0.11% latency, 17.46 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 252.96 us = 0.01% latency, 124.36 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 136.38 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 136.38 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 171.42 us = 0.01% latency, 11.47 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.36 us = 0.01% latency, 5 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 157.36 us = 0.01% latency, 5 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.18 ms = 0.12% latency, 15.69 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 253.92 us = 0.01% latency, 123.89 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.79 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 167.85 us = 0.01% latency, 11.71 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 167.85 us = 0.01% latency, 11.71 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 166.18 us = 0.01% latency, 4.73 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 166.18 us = 0.01% latency, 4.73 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.3 ms = 0.25% latency, 15.46 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 343.8 us = 0.02% latency, 183 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.89 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.89 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 174.05 us = 0.01% latency, 9.04 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 174.05 us = 0.01% latency, 9.04 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 254.39 us = 0.01% latency, 7.73 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 254.39 us = 0.01% latency, 7.73 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 500.92 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 273.7 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 16.66 ms = 0.95% latency, 58.42 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.75 ms = 0.33% latency, 56.37 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.14 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 127.32 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 127.32 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 170.95 us = 0.01% latency, 11.5 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.24 us = 0.01% latency, 31.81 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.24 us = 0.01% latency, 31.81 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.66 ms = 0.21% latency, 88.62 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.06 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.75 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 176.91 us = 0.01% latency, 11.11 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 176.91 us = 0.01% latency, 11.11 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 239.61 us = 0.01% latency, 32.82 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 239.61 us = 0.01% latency, 32.82 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 3.84 ms = 0.22% latency, 84.55 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.33 ms = 0.08% latency, 236.92 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 324.01 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 324.01 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.32 us = 0.01% latency, 39.46 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 199.32 us = 0.01% latency, 39.46 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 243.9 us = 0.01% latency, 8.06 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 243.9 us = 0.01% latency, 8.06 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 249.62 us = 0.01% latency, 123.06 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 604.15 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 610.83 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 600.81 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.87 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
            (33): Gemma3DecoderLayer(
              141.82 K = 2.48% Params, 661 GMACs = 1.17% MACs, 95.65 ms = 5.46% latency, 13.82 TFLOPS
              (self_attn): Gemma3Attention(
                131.58 K = 2.3% Params, 174.39 GMACs = 0.31% MACs, 14.36 ms = 0.82% latency, 24.29 TFLOPS
                (q_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 2.47 ms = 0.14% latency, 26.93 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 400.54 us = 0.02% latency, 157.07 TFLOPS, in_features=2560, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.38 us = 0.01% latency, 11.41 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.38 us = 0.01% latency, 11.41 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 239.61 us = 0.01% latency, 6.56 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 239.61 us = 0.01% latency, 6.56 TFLOPS, in_features=64, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (k_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 1.96 ms = 0.11% latency, 17.44 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 252.96 us = 0.01% latency, 124.36 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 129.7 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 172.85 us = 0.01% latency, 11.37 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 172.85 us = 0.01% latency, 11.37 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 156.88 us = 0.01% latency, 5.01 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 156.88 us = 0.01% latency, 5.01 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (v_proj): lora.Linear(
                  65.54 K = 1.15% Params, 17.1 GMACs = 0.03% MACs, 2.16 ms = 0.12% latency, 15.81 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 15.73 GMACs = 0.03% MACs, 252.25 us = 0.01% latency, 124.71 TFLOPS, in_features=2560, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 126.84 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 126.84 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 167.85 us = 0.01% latency, 11.71 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 167.85 us = 0.01% latency, 11.71 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.21 us = 0.01% latency, 5.07 TFLOPS
                    (default): Linear(65.54 K = 1.15% Params, 393.22 MMACs = 0% MACs, 155.21 us = 0.01% latency, 5.07 TFLOPS, in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (o_proj): lora.Linear(
                  0 = 0% Params, 33.23 GMACs = 0.06% MACs, 4.29 ms = 0.25% latency, 15.5 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 31.46 GMACs = 0.06% MACs, 343.08 us = 0.02% latency, 183.38 TFLOPS, in_features=2048, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 126.36 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 126.36 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 786.43 MMACs = 0% MACs, 171.66 us = 0.01% latency, 9.16 TFLOPS
                    (default): Linear(0 = 0% Params, 786.43 MMACs = 0% MACs, 171.66 us = 0.01% latency, 9.16 TFLOPS, in_features=2048, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 243.66 us = 0.01% latency, 8.07 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 243.66 us = 0.01% latency, 8.07 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (q_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 500.68 us = 0.03% latency, 0 FLOPS, (256,), eps=1e-06)
                (k_norm): Gemma3RMSNorm(256 = 0% Params, 0 MACs = 0% MACs, 272.75 us = 0.02% latency, 0 FLOPS, (256,), eps=1e-06)
              )
              (mlp): Gemma3MLP(
                0 = 0% Params, 486.6 GMACs = 0.86% MACs, 78.11 ms = 4.46% latency, 12.46 TFLOPS
                (gate_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 5.76 ms = 0.33% latency, 56.35 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 222.91 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 128.98 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 183.11 us = 0.01% latency, 10.74 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 183.11 us = 0.01% latency, 10.74 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.81 us = 0.01% latency, 31.99 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 245.81 us = 0.01% latency, 31.99 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (up_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 4.56 ms = 0.26% latency, 71.14 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.41 ms = 0.08% latency, 223.55 TFLOPS, in_features=2560, out_features=10240, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 135.9 us = 0.01% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 135.9 us = 0.01% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 178.58 us = 0.01% latency, 11.01 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 178.58 us = 0.01% latency, 11.01 TFLOPS, in_features=2560, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.96 us = 0.01% latency, 31.72 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 247.96 us = 0.01% latency, 31.72 TFLOPS, in_features=64, out_features=10240, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (down_proj): lora.Linear(
                  0 = 0% Params, 162.2 GMACs = 0.29% MACs, 64.38 ms = 3.68% latency, 5.04 TFLOPS
                  (base_layer): Linear(0 = 0% Params, 157.29 GMACs = 0.28% MACs, 1.32 ms = 0.08% latency, 237.86 TFLOPS, in_features=10240, out_features=2560, bias=False)
                  (lora_dropout): ModuleDict(
                    0 = 0% Params, 0 MACs = 0% MACs, 323.06 us = 0.02% latency, 0 FLOPS
                    (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 323.06 us = 0.02% latency, 0 FLOPS, p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    0 = 0% Params, 3.93 GMACs = 0.01% MACs, 203.13 us = 0.01% latency, 38.72 TFLOPS
                    (default): Linear(0 = 0% Params, 3.93 GMACs = 0.01% MACs, 203.13 us = 0.01% latency, 38.72 TFLOPS, in_features=10240, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    0 = 0% Params, 983.04 MMACs = 0% MACs, 254.87 us = 0.01% latency, 7.71 TFLOPS
                    (default): Linear(0 = 0% Params, 983.04 MMACs = 0% MACs, 254.87 us = 0.01% latency, 7.71 TFLOPS, in_features=64, out_features=2560, bias=False)
                  )
                  (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                  (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                )
                (act_fn): PytorchGELUTanh(0 = 0% Params, 0 MACs = 0% MACs, 250.1 us = 0.01% latency, 122.83 GFLOPS)
              )
              (input_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 601.29 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_attention_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 608.21 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (pre_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 601.29 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
              (post_feedforward_layernorm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 609.64 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
            )
          )
          (norm): Gemma3RMSNorm(2.56 K = 0.04% Params, 0 MACs = 0% MACs, 602.01 us = 0.03% latency, 0 FLOPS, (2560,), eps=1e-06)
          (rotary_emb): Gemma3RotaryEmbedding(0 = 0% Params, 768 KMACs = 0% MACs, 435.59 us = 0.02% latency, 3.53 GFLOPS)
          (rotary_emb_local): Gemma3RotaryEmbedding(0 = 0% Params, 768 KMACs = 0% MACs, 379.8 us = 0.02% latency, 4.04 GFLOPS)
        )
      )
      (lm_head): Linear(0 = 0% Params, 4.03 TMACs = 7.11% MACs, 33.69 ms = 1.93% latency, 239.06 TFLOPS, in_features=2560, out_features=262208, bias=False)
    )
  )
)
------------------------------------------------------------------------------
